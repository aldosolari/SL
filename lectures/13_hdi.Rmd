---
title: 'High-dimensional inference'
output:
  pdf_document: default
  html_document: default
---

Consider the linear model
\begin{eqnarray}\label{lm}
y = \mathcal{N}(1_n \beta_0 + X\beta, \sigma^2I_n)
\end{eqnarray}
where 

* $\underset{n\times 1}{y} = (y_1,\ldots,y_n)'$ is the response on $n$ observations

* $\underset{n\times p}{X}$ is the (fixed or random) design matrix containing the measurements on $p$ variables 

* $\underset{p\times 1}{\beta} = (\beta_1,\ldots,\beta_p)'$ is the vector of coefficients of interest

* $\beta^*_0$ and $\sigma^2$ are nuisance parameters

* $\underset{n\times 1}{1_n} = (1,1,\ldots,1)'$ is a vector of ones of length $n$ and $\underset{n\times n}{I_n}$ is the identity matrix

* We can have a low-dimensional setting ($n \geq p$), but we are particularly interrested in the high-dimensional setting ($n < p$)

The *active set* or the set of relevant variables is defined as
$$S = \{j : \beta_j \neq 0, j=1,\ldots,p\}$$
and it has cardinality $s=\#S$. 

The main goal is the construction of confidence intervals and $p$-values for individual regression parameters $\beta_j$, $j=1,\ldots,p$. In high-dimensional setting, this makes
statistical inference very challenging. 

Here the main assumption is that the linear model in (\ref{lm}) is correct. This might be rather unrealistic. 

## Simulated data 

We use the following simulation setting:

* $n=100$
* $p=200$
* $\beta_0=0$, $\sigma^2 = 1$
* $s=6$ 
* $X \sim \mathcal{N}(0,\Sigma)$
* $\Sigma$ block-diagonal matrix 

We divide the variables in 3 types: A, B and C. Type-$A$ variables have $\beta_j \neq 0$, with two strong effects $\beta_j = \pm 1$
and four weak effects $\beta_j = \pm 0.5$; type-$B$ and $C$ variables have $\beta_j=0$.  Each of the six type-$A$ variables is correlated ($\rho=0.5$) with two other type-$B$ variables. The remaining 42 type-$C$ variables are pure noise and independent of all other variables. 

Avoiding the selection of type-$B$ predictors is challenging; however, simple approaches still work well for avoiding the selection of type-$C$ predictors


```{r}
set.seed(1)
n <- 100
p <- 200
s <- 6
A <- matrix(0.5, 3, 3) + 0.5*diag(3)
B <- diag(p-3*6)
library(Matrix)
Sigma = bdiag(A,A,A,A,A,A,B)
R <- chol(Sigma)
X <- as.matrix(matrix(rnorm(n * p), n, p) %*% R)
beta = numeric(6*3)
beta[(0:5)*3+1] <- c(1,-1,0.5,0.5,-0.5,-0.5)
y <- X[,1:(s*3)] %*% beta + rnorm(n)
varType <- vector("character", p)
varType[(0:5)*3+1] <- "A"
varType[c((0:5)*3+2, (0:5)*3+3)] <- "B"
varType[(s*3+1):p] <- "C"
colnames(X) <- paste0("x", 1:p)
yX = data.frame(y,X)
```




## Single-split inference

A generic way for
deriving $p$-values in hypotheses testing is given by *sample-splitting inference*, that is
splitting the observations with indices $\{1,\ldots,n\}$ in two equal halves denoted by $I_1$ and $I_2$, that is, $I_r \subset \{1,\ldots,n\}$, $r=1,2$ with $I_1 \cup I_2 = \{1,\ldots,n\}$ and
$I_1 \cap I_2=\emptyset$. 

The idea
is to use the first half $I_1$ for variable selection and the
second half $I_2$ with the reduced set of selected variables
(from $I_1$) for statistical inference in terms of $p$-values.


```{r}
set.seed(123)
I1 <- as.logical(sample(rep(0:1, each=n/2)))
```

Such a sample-splitting procedure avoids the
over-optimism to use the data twice for selection and
inference after selection (without taking the effect of
selection into account).

Consider a method for variable selection based on
the first half of the sample:

$$\hat{S}(I_1) \subset \{1,\ldots,p\}$$
A prime example is the Lasso which selects all the variables
whose corresponding estimated regression coefficients
are different from zero.

```{r}
library(glmnet)
set.seed(123)
fit <- cv.glmnet(X[I1,], y[I1])
hatbeta <- coef(fit, s=fit$lambda.min)[-1]
tapply(hatbeta!=0, varType, sum)
```

We then use the second
half of the sample $I_2$ for constructing $p$-values,
based on the selected variables $\hat{S}(I_1)$.

If the cardinality $\# \hat{S}(I_1) \leq n/2$, we can run ordinary least
squares estimation using the subsample $I_2$ and the
selected variables $\hat{S}(I_1)$, that is, we regress $y_{I_2}$ on $X_{I_2}^{\hat{S}(I_1)}$
where the sub-indices denote the sample half
and the super-index stands for the selected variables,
respectively. 

```{r}
XS <- X[!I1, which(hatbeta!=0)]
fit <- lm(y[!I1]~XS)
summary(fit)
```

Thus, from such a procedure, we obtain $p$-values for testing
$H_j: \beta_j = 0$ for $j \in \hat{S}(I_1)$. Moreover,
we define (raw) $p$-values $p_j=1$ for $j \notin \hat{S}(I_1)$. 

```{r}
p.raw = rep(1,p)
var.id <- as.numeric(gsub("XSx", "", rownames(summary(fit)$coef)[-1] ) )
var.id
p.raw[var.id] = summary(fit)$coefficients[-1,4]
tapply(p.raw <= 0.05, varType, sum)
```

An interesting feature of such a sample-splitting procedure
is the adjustment for multiple testing. For example,
if we wish to control the familywise error rate
over all considered hypotheses $H_j$, $j =1,\ldots,p$, a
naive approach would employ a Bonferroniâ€“Holm correction
over the $p$ tests. This is not necessary: we only
need to control over the considered $\# \hat{S}(I_1)$ tests in $I_2$. 

```{r}
p.holm = p.adjust(p.raw[var.id], "holm")
names(p.holm) = paste("x", var.id, sep="")
round(p.holm,4)
```

Such corrected $p$-values control
the familywise error rate in multiple testing when assuming
the *screening property*
$$\hat{S}=\{j: \hat{\beta}_j\neq 0\} \supseteq S$$

for the selector $\hat{S} = \hat{S}(I_1)$ based on the first half $I_1$ only. The reason is that the screening
property ensures that the reduced model is a correct
model, and hence the result is not surprising.

## Multiple-split inference

A major problem of the single sample-splitting
method is its sensitivity with respect to the choice
of splitting the entire sample: sample splits lead to
wildly different $p$-values. We call this undesirable phenomenon
a $p$-value *lottery*, and the following histogram provides an
illustration for a single variable. 

```{r}
set.seed(123)
B = 50
P <- matrix(1, B, p)
for (i in 1:B) {
  ind <- as.logical(sample(rep(0:1, each=n/2)))
  cvfit <- cv.glmnet(X[ind,], y[ind])
  b <- coef(cvfit, s=cvfit$lambda.min)[-1]
  XX <- X[!ind, which(b!=0)]
  fit <- lm(y[!ind]~XX)
  summ <- summary(fit)$coefficients[-1,]
  var.id <- as.numeric(gsub("XXx", "", rownames(summ)))
  P[i, var.id] <- summ[,4]
}
hist(P[,1])
```


To overcome the $p$-value lottery, we can run the sample-splitting method $B$ times, with $B$ large.

```{r}
boxplot(apply(P, 2, median)[(0:5)*3+1],
        apply(P, 2, median)[c((0:5)*3+2, (0:5)*3+3)],
        apply(P, 2, median)[(s*3+1):p], col="gray", frame.plot=FALSE, pch=19,
        names=LETTERS[1:3], las=1, ylim=c(0,1), ylab="p")
```


Thus, we obtain a collection of $p$-values for the $j$th hypothesis $H_j$
$$p_j^{[1]},\ldots,p_j^{[B]}$$
The task is now to do an aggregation to a single $p$-value. Because of dependence among $\{p_j^{[B]},b=1,\ldots,B\}$, because all the different half samples are
part of the same full sample, an appropriate aggregation is needed.

A simple solution is to use the median of $\{p_j^{[B]},b=1,\ldots,B\}$ and multiplying it with the factor 2.

```{r}
p.median = apply(P,2,median)
tapply(p.median <= 0.05, varType, sum)
```

The implementation in the R package hdi works as follow

```{r, comment=FALSE, message=FALSE}
library(hdi)
set.seed(123)
fit <- multi.split(x=X, y=y, B=50, fraction=0.5, ci=TRUE, ci.level = 0.95)
fit
```

To obtain adjusted $p$-values for $H_j$ controlling the familywise error rate:

```{r}
p.fwer = fit$pval.corr
round(p.fwer,4)
```


Confidence intervals can be constructed based on the
duality with the $p$-values:

```{r}
confint(fit, parm=which(p.fwer <= 0.05), level=0.95)
```

