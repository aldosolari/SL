---
title: "Two inferential problems"
output:
  pdf_document: default
  html_document: default
---

Consider the linear model
$$ y = \mathcal{N}(1_n \beta_0 + X\beta, \sigma^2I_n)$$
where 

* $\underset{n\times 1}{y} = (y_1,\ldots,y_n)'$ is the response on $n$ observations

* $\underset{n\times p}{X}$ is the design matrix containing the measurements on $p$ variables 

* $\underset{p\times 1}{\beta} = (\beta_1,\ldots,\beta_p)'$ is the vector of coefficients of interest

* $\beta_0$ and $\sigma^2$ are nuisance parameters

* $\underset{n\times 1}{1_n} = (1,1,\ldots,1)'$ is a vector of ones of length $n$ and $\underset{n\times n}{I_n}$ is the identity matrix



## Multiple testing

The first inferential problem concerns multiple testing, in particular what happens when we perform simultaneously a large number of tests. 

In the linear model described above, we can for example consider testing 
$$H_j: \beta_j = 0 \quad \mathrm{vs} \quad \bar{H}_j: \beta_j \neq 0, \qquad j = 1,\ldots,p$$

If we reject the null hypothesis $H_j$, then we can say that the $j$th variable is `important' in explaing the response. 

Consider the following scenario:

* $n=100$, $p=25$

* $X$ with generic element $x_{ij} \sim U(0,1)$, $\beta_0=2$

* $\beta_j=1$ if $j\in\{1,2,\ldots,5\}$ and $\beta_j=0$ for $j \in \{6,\ldots,25\}$

thus only the first 5 variables are important. 

Now we generate the data:

```{r}
rm(list=ls())
set.seed(123)
n = 100
p = 25
# design matrix
X = matrix(runif(n*p), ncol=p)
colnames(X) = paste0("X",1:p)
# betas
beta = c(rep(1,5),rep(0,p-5))
# response
y = 2 + X %*% beta + rnorm(n)
# data
yX = data.frame(y,X)
# linear model
fit <- lm(y~., yX)
summary(fit)
```

If we reject all the null hypotheses which have $p$-values less than $5\%$, we commit 3 type I errors: 

```{r}
# type I errors
names(which(summary(fit)$coef[-c(1:6),4] < 0.05))
```

We would like to avoid to conclude that X6, X11 and X20 are important variables.

## Inference after model selection

This is our second inferential problem. Inference after model selection was typically done ignoring the model selection process.

Consider the following *high-dimensional* scenario:

* $n=25$, $p=100$

* $X$ with generic element $x_{ij} \sim U(0,1)$, $\beta_0=2$

* $\beta_j=1$ if $j\in\{1,2,\ldots,5\}$ and $\beta_j=0$ for $j \in \{6,\ldots,25\}$

thus only the first 5 variables are important. 

Generate the data:

```{r}
rm(list=ls())
set.seed(123)
n = 25
p = 100
# design matrix
X = matrix(runif(n*p), ncol=p)
colnames(X) = paste0("X",1:p)
# betas
beta = c(rep(2,5),rep(0,p-5))
# response
y = 2 + X %*% beta + rnorm(n)
# data
yX = data.frame(y,X)
```

Perform the forward selection algorithm and select the model with 5 variables:

```{r}
fml.full <- as.formula(paste("y ~ ", paste(colnames(X), collapse= "+")))
fit.null <- lm(y~1,yX)
fit.fwd <- step(fit.null, scope=fml.full, direction="forward", steps=5, trace=0)
# selected variables
S.fwd <- attr(fit.fwd$coefficients, "names")[-1]
S.fwd
```

None of the selected variables is important. But if we fit the linear model with the selected variables, something is going wrong with the inference on the selected model

```{r}
# inference after model selection
fml.fwd <- as.formula(paste("y ~ ", paste(S.fwd, collapse= "+")))
summary(lm(fml.fwd,yX))
```

Alternatively, we can use the LASSO algorithm to select the 5 variables, but the same problem happens: 

```{r, message=FALSE, warning=FALSE}
library(glmnet)
fit.lasso <- glmnet(X,y, alpha=1, dfmax=5)
lambda = fit.lasso$lambda[which.max(fit.lasso$df>=5)]
# selected variables
S.lasso = colnames(X)[which(coef(fit.lasso, s=lambda)[-1]!=0)]
S.lasso

# inference after model selection
fml.lasso <- as.formula(paste("y ~ ", paste(S.lasso, collapse= "+")))
summary(lm(fml.lasso,yX))
```

However, the problem with inference on the selected model seems to disappear if we select the 5 variables at random:

```{r}
# selected variables
S.random <- sample(colnames(X),5)
S.random

# inference after model selection
fml.random <- as.formula(paste("y ~ ", paste(S.random, collapse= "+")))
fit.random <- lm(fml.random,yX)
summary(fit.random)

# confidence intervals
confint(fit.random)[-1,]
```

## References 

* Efron and Hastie (2016) Computer-Age Statistical Inference: Algorithms, Evidence, and Data Science, Cambridge University Press

