---
title: "**Statistical Learning**"
output:
  html_document:
    theme: cosmo
    toc: yes
    toc_float: yes
---

Teacher: [Aldo Solari](https://aldosolari.github.io/)

*Big Data* and *Data Science* have now become recurring terms in media communication. In response to the growing need to analyze data, the profession of the *data scientist* has recently emerged. This course aims to provide new tools for the data scientist. These tools require technical skills such as programming and statistics. They also require professional skills such as communication, teamwork, problem solving, and critical thinking. See the [course syllabus](https://aldosolari.github.io/SL/syllabus/syllabus.html) for further information.

The topics covered by this course are

* *Advanced statistical/machine learning models*, e.g. GAMs, boosting algorithms, support vector machines
* *Large-scale testing* and *selective inference*
* *Modern inferential methods for the analysis of high-dimensional data*, e.g.  sample-splitting inference, stability selection, and knockoffs

The final project is to do a novel data analysis and write a blog post about it. See the [#Project](#project) section for more details.


| When | Where | Hours |
|---|---|----|
| Monday | Lab713 | 13:30-16:30 |
| Wednesday | Lab713 | 15:30-18:30|
| Thursday | Lab713 | 12:30-14:30|


# **Latest announcements**

* WebEx room: 

# **Course Material**

Most of the course material can be found in the notes linked to below. The notes are supplemented by readings which are listed in the [#Reading](#reading) section.

| N | Date |  Lecture |  Slides | 
|--|--|--------------|--------|
| 1 | Dicember 3 | Introduction | [Slides](docs/slides/1_Intro/1_Intro.html) |  
| 2 | Dicember 5 | Regression and smoothing splines | [Slides](docs/slides/2_Splines/2_Splines.html) | 
| 3 | Dicember 6 | Generalized additive models | [Slides](docs/slides/3_GAM/3_GAM.html) | 
||||
| 4 | Dicember 10 | Boosting with Squared-Error Loss | [Slides](docs/slides/4_L2boost/4_L2boost.html) | 
| 5 | Dicember 12 | Boosting algorithms | Slides | 
| 6 | Dicember 13 | tbd | Slides | 
|||||
| 7 | January 7 | tbd | Slides | 
| 8 | January 9 | tbd | Slides | 
| 9 | January 10 | tbd | Slides | 
|||||
| 10 | January 14 | tbd | Slides | 
| 11 | January 16 | tbd | Slides | 
| 12 | January 17 | tbd | Slides | 
|||||
| 13 | January 21 | tbd | Slides | 
| 14 | January 23 | tbd | Slides | 
| 15 | January 24 | tbd | Slides | 
|||||
| 16 | January 28 | tbd | Slides |  

All of the course material is on the [github repo](https://github.com/aldosolari/SL). 

# **Reading**

There are two primary references:

* [Computer-Age Statistical Inference](https://web.stanford.edu/~hastie/CASI/) (CASI)
* [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/) (ESL)

By class:

**December 3** Introduction

  - CASI 1, 1.1 (pages 3-8), 4.3 (pages 45-46)

**December 5** Regression and smoothing splines

  - ESL 5, 5.1, 5.2, 5.2.1, 5.2.2, 5.4, 5.4.1
  
**December 6** Generalized additive models

  - ESL 5.7, 9.1, 9.1.1, 9.1.2, 9.1.3

**December 10** Boosting with squared-error loss
  - CASI 17.2, 
  
**December 13** Boosting algorithms
  - CASI 17.3, 17.4, 17.5



# **Exam**

Exams schedule:

|When | Where | Hour | Type |
|---|---|---|---|
| 8/2/2019 | U7/13 | 14:00 | Written and Oral |
| 21/2/2019 | U7/13 | 14:00 | Written and Oral |

The exam consists of two parts:

- __Written exam__ (open-ended and/or closed-ended questions)

- __Oral exam__, including the __presentation__ of a data science __project__ (homework)
    - Attending students can form a __team__ (max. 3 persons) and present at the end of the course. In this case the oral exam is optional

The final grade is a weighted average of written exam (1/2) and oral exam (1/2). See the [course syllabus](https://aldosolari.github.io/SL/syllabus/syllabus.html) for further information.




# **Project**

Your final project is to do a novel data analysis to answer a question and write a blog post about it on github. It should answer:

1. What is the question(s) you tried to answer? Why should someone care?

2. What is the data/how did you get it?

3. How did you answer the questions (e.g. what statistical techniques, etc)?

4. What are your findings?

The following references may help with the project:

* [Data Science in R: A Case Studies Approach to Computational Reasoning and Problem Solving](http://rdatasciencecases.org/) 
* [Modern Data Science with R](https://mdsr-book.github.io/) 
* [FiveThirtyEight](https://fivethirtyeight.com/)
* [Normal Deviate](https://normaldeviate.wordpress.com/)

More informations will be available soon.


This page was last updated on `r Sys.time()` Eastern Time.

