<!DOCTYPE html>
<html>
  <head>
    <title>Splines</title>
    <meta charset="utf-8">
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <strong>Splines</strong>
### Aldo Solari

---





# Outline


* Regression splines

* Natural cubic splines

* Smoothing splines

---

# Moving beyond linearity

In many data situations, the relationship between `\(x\)` and `\(y\)` is not linear. 
Common approaches in regression to deal with nonlinear relationships are

* Polynomial regression

* Step functions

* Regression splines

* Smoothing splines

* Generalized additive models

* Local regression (lowess)

---

# Polynomial regression may be problematic

* Suppose 
`$$y = f(x) + \varepsilon$$`

* Consider the following functional form for `\(x\)`
`$$f(x) = sin(2(4x-2)) + 2e^{-(16^2)(x-.5)^2}$$`

* Even with a polynomial of degree 15, the fit is fairly poor in many areas, and 'wiggles' in some places where there doesn't appear to be a need to

---


```r
set.seed(123)
x = sort(runif(500))
mu = sin(2*(4*x-2)) + 2*exp(-(16^2)*((x-.5)^2))
y = rnorm(500, mu, .3)
plot(x,y ,col="lightgray")
lines(x,mu, col=2, lwd=2)
lines(x, fitted(lm(y ~ poly(x,15))) )
```

&lt;img src="2_Splines_files/figure-html/unnamed-chunk-1-1.png" width="60%" style="display: block; margin: auto;" /&gt;


---

# Piecewise polynomial

* So how might we solve the problem we saw with polynomial regression? 

* One way would be to divide the data into chunks at various points (__knots__), and fit a polynomial model within that subset of data

* Define `\(K\)` knots (internal breakpoints) on the range of `\(x\)`:
`$$\min(x)&lt;\xi_1&lt; \ldots &lt; \xi_K&lt;\max(x)$$`

* Fit a polynomial model on each of the `\(K+1\)` intervals 

`$$(-\infty, \xi_{1}], (\xi_{1}, \xi_{2}], \ldots, (\xi_{K-1}, \xi_{K}], (\xi_{K}, +\infty)$$`

---


```r
knots = seq(0.1, 0.9, by=.1)
xcut = cut(x, c(-Inf,knots,Inf) )
plot(x,y ,col="lightgray")
lines(x, mu, col=2, lwd=2)
abline(v=knots, lty=2)
for (i in 1:length(levels(xcut)) ){
sub = (xcut==levels(xcut)[i])
lines(x[sub], fitted(lm(y[sub] ~ poly(x[sub],3))) )
}
```

&lt;img src="2_Splines_files/figure-html/unnamed-chunk-2-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---

# Step function

&lt;img src="2_Splines_files/figure-html/unnamed-chunk-3-1.png" width="60%" style="display: block; margin: auto;" /&gt;


---

# Piecewise linear regression

&lt;img src="2_Splines_files/figure-html/unnamed-chunk-4-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---

# Basis functions

Consider a general basis expansion for `\(x\)`:

`$$f(x) = \sum_{j=1}^{p} \beta_j  b_j(x)$$`
where `\(b_j(\cdot)\)` are known functions called __basis functions__. For example

* 3rd degree polynomial: 
`$$b_1(x) = 1, b_2(x)= x, b_{3}(x) = x^2,  b_{4}(x) = x^3$$`

* Step function with `\(K\)` knots: 
`$$b_1(X) = I\{x &lt; \xi_1\}, b_2(x) = I\{\xi_1 \leq x &lt; \xi_2\} ,\ldots, b_{K}(x) = I\{\xi_{K-1} \leq x &lt; \xi_K\},  b_{K+1}(x) = I\{ x \geq \xi_K\}$$`

* Piecewise linear with `\(K\)` knots:

`$$b_1(x) =I\{x &lt; \xi_1\}, 
b_2(x) = x \cdot I\{x &lt; \xi_1\},
b_3(x) = I\{\xi_1 \leq x &lt; \xi_2\}, 
b_4(x) = x \cdot I\{\xi_1 \leq x &lt; \xi_2\},\ldots, 
b_{2(K+1)}(x) = x \cdot I\{ x \geq \xi_K\}$$`

---

# Regression splines

* A __spline__ of __degree__ `\(d\)` with knots `\(\xi_1,\ldots,\xi_K\)` is a
piecewise-polynomial function of degree `\(d\)`, and it has continuous derivatives up to order `\(d-1\)` at each knot

* It is defined by the __truncated power basis__:
`$$b_j(x) = x^{j-1}, \quad j=1,\ldots,d+1$$`
`$$b_{d+k+1}(x) = (x - \xi_k)_+^{d}, \quad k=1,\ldots, K$$`
where `\((\cdot)_+\)` defines the positive portion of its argument, i.e. 

`$$(x - \xi_k)^d_+= \begin{cases} 
(x-\xi_k)^d &amp; x \geq \xi_k \\ 
0 &amp; \textrm{otherwise}
\end{cases}$$`

* While the truncated power
basis is conceptually simple, it is not too attractive numerically: powers of
large numbers can lead to severe rounding problems

* In contrast, the `\(B\)`-spline basis allows for efficient computations

* The fixed-knot splines are also known as __regression splines__

---


```r
d = 1
K = length(knots)
n = length(x)
bs = matrix(NA,ncol=d+K+1, nrow=n)
bs[,1:(d+1)] = cbind(1,poly(x,d, raw=TRUE))
bs[,(d+2):(d+K+1)] = sapply(1:length(knots), function(k) ifelse(x &gt;= knots[k], (x-knots[k])^d, 0))
```

---


```r
plot(x,y ,col="lightgray")
abline(v=knots, lty=2)
for (i in 1:ncol(bs)) lines(x,bs[,i], col=i)
```

&lt;img src="2_Splines_files/figure-html/unnamed-chunk-6-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---


```r
fit = lm(y ~ .-1, data=data.frame(bs))
betas = coef(fit)
betas
```

```
         X1          X2          X3          X4          X5          X6          X7          X8          X9         X10         X11 
  0.8041857  -6.6191752  -1.9833654   4.6377841   8.8004403  22.9154212 -40.7618274  13.9615059  -2.9885924  -6.9779049   4.8408884 
```

```r
bsScaled = sapply(1:length(betas),function(j) bs[,j]*betas[j])
```

---


```r
plot(x,y ,col="lightgray")
abline(v=knots, lty=2)
for (i in 1:ncol(bsScaled)) lines(x,bsScaled[,i], col=i)
```

&lt;img src="2_Splines_files/figure-html/unnamed-chunk-8-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---


```r
plot(x,y ,col="lightgray")
lines(x, mu, col=2, lwd=2)
abline(v=knots, lty=2)
fitted = apply(bsScaled, 1, sum)
lines(x,fitted)
```

&lt;img src="2_Splines_files/figure-html/unnamed-chunk-9-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---

# Preview of B-splines basis

&lt;img src="2_Splines_files/figure-html/unnamed-chunk-10-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="2_Splines_files/figure-html/unnamed-chunk-11-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---

# Cubic splines


```r
library(splines)
plot(x,y ,col="lightgray")
lines(x, mu, col=2, lwd=2)
lines(x, fitted( lm(y ~ bs(x, degree=3, knots=knots)) ) )
```

&lt;img src="2_Splines_files/figure-html/unnamed-chunk-12-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---

# Natural cubic splines

* One problem with regression splines is that the estimates
have high variance at the boundaries

* A way to remedy this problem is to force the piecewise
polynomial function to have lower degree to the left of the
leftmost knot, and to the right of the rightmost knot

* A __natural cubic spline__ with knots `\(\xi_1,\ldots,\xi_K\)` is a piecewise polynomial function `\(f\)` such that
    - `\(f\)` is a cubic polynomial on each `$$[\xi_1,\xi_2],[\xi_2,\xi_3],\ldots,[\xi_{K-1},\xi_{K}]$$`

    - `\(f\)` is linear on `\((-\infty, \xi_1]\)` and `\([\xi_K,\infty)\)`
    -  `\(f\)` is continuous and has continuous derivatives `\(f'\)` and `\(f''\)` at each knot `\(\xi_1,\ldots,\xi_K\)`: `\(f(\xi_k^-)=f(\xi_k^+)\)`, `\(f'(\xi_k^-)=f'(\xi_k^+)\)` and `\(f''(\xi_k^-)=f''(\xi_k^+)\)`, `\(k=1,\ldots,K\)`, where `\(\xi_k^+\)` and `\(\xi_k^-\)` indicate indicate the left and right limits of the function `\(f(\cdot)\)` at `\(\xi_k\)`


---


```r
library(splines)
plot(x,y ,col="lightgray")
lines(x, mu, col=2, lwd=2)
lines(x, fitted( lm(y ~ ns(x, knots=knots)) ) )
```

&lt;img src="2_Splines_files/figure-html/unnamed-chunk-13-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---

# Degrees of freedom

* The number of degrees of freedom consumed by a natural cubic spline is
`$$\underbrace{4\cdot (K-1)}_{a} + \underbrace{2\cdot 2}_b - \underbrace{3\cdot K}_c = K$$` 
where 
  - `\(a\)` is the number of free parameters in the interior intervals  `\([\xi_1,\xi_2],[\xi_2,\xi_3],\ldots,[\xi_{K-1},\xi_{K}]\)`
  - `\(b\)` is the number of free parameters in the exterior intervals `\((-\infty, \xi_1]\)` and `\([\xi_K,\infty)\)`
  - `\(c\)` is the number of constraints at the knots `\(\xi_1,\ldots,\xi_K\)`

* Natural cubic splines are very useful tools, but they do have one
shortcoming: deciding the number and the placement of the knots

---

# The number of knots and where to put them

&lt;img src="2_Splines_files/figure-html/unnamed-chunk-14-1.png" width="60%" style="display: block; margin: auto;" /&gt;


---

# Controlling smoothness with penalization


We can avoid the knot selection problem altogether by formulating a penalized minimization problem

 `$$\underset{f \in \mathcal{F}''} \min \,\, \left\{ \sum_{i=1}^{n}(y_i - f(x_i))^2 + \lambda \int \{f''(t)\}^2 dt \right\}$$`
where `\(f\)` belongs to the family `\(\mathcal{F}''\)` of  twice-differentiable functions

* The first term is RSS, and tries to make `\(f(x_i)\)` match `\(y_i\)` at each `\(x_i\)`

* The second term is a __roughness penalty__ and controls how
wiggly `\(f(x)\)` is

* The penalty is modulated by the __tuning parameter__ `\(\lambda\geq 0\)`:
  - `\(\lambda=0\)` imposes no restrictions and `\(f\)` will therefore interpolate
the data 
  -  `\(\lambda = \infty\)` renders curvature impossible, and the function `\(f\)` becomes linear

---

# Theorem

* It may sound impossible to solve the penalized minimization problem for `\(f\)` over all possible functions in `\(\mathcal{F}''\)`, but the solution turns out to be
surprisingly simple: it must be a __natural cubic spline__

* __Theorem__ (Green and Silverman, 1994)

&gt; Out of all twice-differentiable functions `\(f\)`, the one that minimizes 
 `$$\sum_{i=1}^{n}(y_i - f(x_i))^2 + \lambda \int \{f''(t)\}^2 dt$$`
is a natural cubic spline with knots at every unique value of `\(x_i\)`


---

# Matrix formulation

* Consider the natural cubic spline
`$$f(x) = \sum_{j=1}^{K} \beta_j b_j(x)$$` 
where  `\(b_j(\cdot)\)` are natural cubic spline basis functions and
`\(K\leq n\)` is the number of unique values of `\(x_i\)` (knots)

* Define the __basis matrix__ `\(\underset{n \times K}{\mathbf{N}}\)` with elements
`$$N_{ij} = b_j(x_i), \quad i=1,\ldots,n, \quad  j=1,\ldots,K$$`

* Define the __penalty matrix__ `\(\underset{K \times K}{\mathbf{\Omega}}\)` with elements
`$$\Omega_{ij} = \int b''_i(t) b''_j(t)dt$$`
---

# Smoothing splines

* The penalised minimization problem can be written as 
`$$\hat{\boldsymbol{\beta}}^\lambda = \underset{\boldsymbol{\beta} }{\arg \min}\, \{ \| \mathbf{y} - \mathbf{N}\boldsymbol{\beta}\|^2_{\ell_2} + \lambda \boldsymbol{\beta}^\mathsf{T}\mathbf{\Omega} \boldsymbol{\beta} \}$$`
showing that is type of generalized ridge regression problem

* The solution has an explicit form: 
`$$\hat{\boldsymbol{\beta}}^\lambda = ( \mathbf{N}^\mathsf{T}   \mathbf{N} + \lambda \mathbf{\Omega} )^{-1} \mathbf{N}^\mathsf{T}  \mathbf{y}$$`

---

# Selection of `\(\lambda\)`

* Smoothing spline estimates are linear 
`$$\hat{\mathbf{y}} = \mathbf{N} ( \mathbf{N}^\mathsf{T}   \mathbf{N} + \lambda \mathbf{\Omega} )^{-1} \mathbf{N}^\mathsf{T}  \mathbf{y}
= \mathbf{H}^\lambda \mathbf{y}$$`
where `\(\mathbf{H}^\lambda\)` is the __smoothing matrix__

* Selection of `\(\lambda\)` may be based on `\(\mathrm{trace}(\mathbf{H}^\lambda)\)`
often referred as to the "equivalent degrees of freedom" 

* LOOCV selects the value of  `\(\lambda\)` which minimizes 
`$$\sum_{i=1}^{n}( y_i - \hat{f}^{-i}_{\lambda}(x_i))^2 =  \sum_{i=1}^{n} \left( \frac{y_i - \hat{f}_{\lambda}(x_i)}{1- \{\mathbf{H}^\lambda\}_{ii}}\right)^2$$`

* __Generalized cross-validation__ selects the value of `\(\lambda\)` which minimizes 
`$$\sum_{i=1}^{n} \left( \frac{y_i - \hat{f}_{\lambda}(x_i)}{1-  \mathrm{trace}(\mathbf{H}^\lambda)/n } \right)^2$$`

---


```r
fit = smooth.spline(x, y, lambda=0)
plot(x,y ,col="lightgray")
lines(x, predict(fit)$y)
```

&lt;img src="2_Splines_files/figure-html/unnamed-chunk-15-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---


```r
# LOOCV
fit = smooth.spline(x, y, cv=TRUE)
plot(x,y ,col="lightgray")
lines(x, mu, col=2, lwd=2)
lines(x, predict(fit)$y)
```

&lt;img src="2_Splines_files/figure-html/unnamed-chunk-16-1.png" width="60%" style="display: block; margin: auto;" /&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
