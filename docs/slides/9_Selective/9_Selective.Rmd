---
title: "**Selective Inference**"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true    
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, 
                      comment=NA, cache=F, R.options=list(width=220),
                      fig.align='center', out.width='60%', fig.asp=.7)
```


# Outline

* A new scientific paradigm

* Confidence Intervals for Selected Parameters

---

# A new scientific paradigm

.pull-left[

**Textbook practice**

1. Select hypotheses/models/questions

2. Collect data

3. Perform inference

]

.pull-right[

**Modern practice**

1. Collect data

2. Select hypotheses/models/questions

3. Perform inference

]

---

# Hypothesis-driven research

1. The researcher has a theory/hypothesis/model

2. The researcher designs an experiment about this theory/hypothesis/model and collects data

3. Experimental data provide means to falsify the theory/hypothesis/model and/or to
formulate a better theory/hypothesis/model

---

# How Things Fall

* Researcher: Galileo Galilei (1565-1642)

* Question of interest: If a ball rolls down a ramp, what is the relationship between *time*
and *distance*?

* Aristotle theory/hypothesis/model: *Constant velocity* (zero acceleration): distance $\propto$ time
$$d = \theta t + \varepsilon$$

* Galileo theory/hypothesis/model: *Increasing velocity* (constant acceleration): distance $\propto$ time $^2$

$$d = \theta t + \gamma t^2 + \varepsilon$$

* Galileo's goal: reject Aristotle theory/hypothesis/model, i.e. the null hypothesis $H_0: \gamma = 0$


---

# Inclined plane experiment (1604)

![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/76/Piano_inclinato_inv_1041_IF_21341.jpg/220px-Piano_inclinato_inv_1041_IF_21341.jpg)

Experimental data: 

```{r, echo=FALSE}
# data
time = 1:8
distance = c(33, 130, 298, 526, 824, 1192, 1620, 2104)
rolling = data.frame(time, distance)
knitr::kable(t(rolling), format="html")
```


See [Galileo's Great Discovery: How Things Fall](https://www.springer.com/cda/content/document/cda_downloaddocument/9781461454434-c1.pdf?SGWID=0-0-45-1366410-p174596162)

---

# Aristotle vs Galileo

```{r, echo=FALSE}
# data
time = 1:8
distance = c(33, 130, 298, 526, 824, 1192, 1620, 2104)
rolling = data.frame(distance, time)
# aristotle model
fit0 = lm(distance ~ 0 + time, rolling)
# galileo model
fit1 = lm(distance ~ 0 + time + I(time^2) , rolling)
plot(distance ~ time, rolling)
abline(fit0)
lines(time, fitted(fit1), col=2)
# Null hypothesis rejected with p-value = 9.81e-13
```



---

# Data-driven research

* Very different from hypothesis-driven research

* Large data sets available prior to formulation of hypotheses/model/questions

* Elementary stats textbooks and researchers often ignore selection issues. As a consequence, inference may be wrong and misleading: see the example [The U.S. economy is affected by whether Republicans or Democrats are in office](https://fivethirtyeight.com/features/science-isnt-broken/#part1)

* Need to quantify the reliability of hypotheses/model/questions generated by *data snooping*:
account for *look-everywhere effect*

---

# Confidence intervals 


* So far, we have been studying multiple hypothesis testing

* Today we will look at the other inference
problem, namely, multiple confidence intervals and coverage after selection

* Consider a classical setting in which we have $m$ parameters $\theta_1, \theta_2, \ldots, \theta_m$ and corresponding estimators 
$$T_i \sim N(\theta_i,1), \qquad i=1,\ldots,m$$

* In this setting, we can construct $1-\alpha$ confidence intervals
$$\mathrm{CI}_i = [T_i - z_{1-\alpha/2}, T_i + z_{1-\alpha/2}], \qquad i=1,\ldots,m$$

* Each interval has *marginal coverage*
$$\Pr(\theta_i \in \mathrm{CI}_i ) = 1-\alpha, \qquad i=1,\ldots,m$$

---

# Average coverage

*  Ignoring the *multiplicity* of confidence intervals (CIs) is generally more common than ignoring the problem of multiplicity in testing

* One reason why unadjusted CIs seem more acceptable than unadjusted tests is that they give the right coverage *on average*

* The proportion of $1-\alpha$ CIs covering their respective parameters out of the intervals constructed is expected to be $1-\alpha$
$$\mathbb{E}\left( \frac{1}{m} \sum_{i=1}^{m} I\{ \theta_i \in \mathrm{CI}_i \} \right) =  \frac{1}{m} \sum_{i=1}^{m} \Pr( \theta_i \in \mathrm{CI}_i )  = 1-\alpha$$
 and thus only $\alpha$ will not be covered. So why worry?

---

# Coverage after selection


* Often researchers examine many parameters at once and report confidence intervals for *selected ones*

* However, as noted by Soric in 1989:

> In a large number of 95% confidence intervals, 95% of them contain the population
parameter [...] but it would be wrong to imagine that the same rule also applies to a
large number of 95% interesting confidence intervals

---

# Example

* Generate parameters $\theta_i$ i.i.d. $N(0,0.04)$, $i=1,\ldots, 20$

* Assume $T_i \sim N(\theta_i,1)$, $i=1,\ldots, 20$

* Construct 90% confidence intervals $\mathrm{CI}_i = [T_i - 1.64, T_i + 1.64]$

* Select interesting parameters whose CIs do not cover 0

* Compute the coverage proportion of CIs for all parameters

* Compute the coverage proportion of CIs for selected parameters

---

# False coverage rate

* **False coverage rate** (FCR) is defined as
$$\mathrm{FCR} = \mathbb{E}\left( \frac{V_{\mathrm{CI}}}{\max(R_{\mathrm{CI}},1)}\right)$$
where $R_{\mathrm{CI}}$ is the number of selected parameter and $V_{\mathrm{CI}}$ the number of constructed confidence intervals not covering

* It is similar to FDR in that it controls type I error over the selected parameters

* Without selection, the marginal CIs control the FCR







