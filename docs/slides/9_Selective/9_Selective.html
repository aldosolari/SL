<!DOCTYPE html>
<html>
  <head>
    <title>Selective Inference</title>
    <meta charset="utf-8">
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <strong>Selective Inference</strong>
### Aldo Solari

---





# Outline

* A new scientific paradigm

* The law of selection

* Confidence intervals for selected parameters

---

layout: false
class: inverse, middle, center

# A new scientific paradigm

---

# A new scientific paradigm

.pull-left[

**Textbook practice**

1. Select hypotheses/models/questions

2. Collect data

3. Perform inference

]

.pull-right[

**Modern practice**

1. Collect data

2. Select hypotheses/models/questions

3. Perform inference

]

---

# Hypothesis-driven research

1. The researcher has a theory/hypothesis/model

2. The researcher designs an experiment about this theory/hypothesis/model and collects data

3. Experimental data provide means to falsify the theory/hypothesis/model and/or to
formulate a better theory/hypothesis/model

---

# How Things Fall

* Researcher: Galileo Galilei (1565-1642)

* Question of interest: If a ball rolls down a ramp, what is the relationship between *time*
and *distance*?

* Aristotle theory/hypothesis/model: *Constant velocity* (zero acceleration): distance `\(\propto\)` time
`$$d = \theta t + \varepsilon$$`

* Galileo theory/hypothesis/model: *Increasing velocity* (constant acceleration): distance `\(\propto\)` time `\(^2\)`

`$$d = \theta t + \gamma t^2 + \varepsilon$$`

* Galileo's goal: reject Aristotle theory/hypothesis/model, i.e. the null hypothesis `\(H_0: \gamma = 0\)`

* Note that Galileo's null hypothesis is fixed a priori, i.e. before seeing the data

---

# Inclined plane experiment (1604)

![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/76/Piano_inclinato_inv_1041_IF_21341.jpg/220px-Piano_inclinato_inv_1041_IF_21341.jpg)

Experimental data: 

&lt;table&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; time &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; distance &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 33 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 130 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 298 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 526 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 824 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1192 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1620 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2104 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


See [Galileo's Great Discovery: How Things Fall](https://www.springer.com/cda/content/document/cda_downloaddocument/9781461454434-c1.pdf?SGWID=0-0-45-1366410-p174596162)

---

# Aristotle vs Galileo

&lt;img src="9_Selective_files/figure-html/unnamed-chunk-2-1.png" width="60%" style="display: block; margin: auto;" /&gt;



---

# Data-driven research

* Very different from hypothesis-driven research

* Large data sets available prior to formulation of hypotheses/model/questions

* Elementary stats textbooks and researchers often ignore selection issues. As a consequence, inference may be wrong and misleading

* Need to quantify the reliability of hypotheses/model/questions generated by *data snooping*: account for *look-everywhere effect*

---

# Is U.S. economy affected by political power?

* https://projects.fivethirtyeight.com/p-hacking/index.html

.pull-left[

* $Y = $ Economic performance 
    - `\(Y_1\)` Employment
    - `\(Y_2\)` Inflation
    - `\(Y_3\)` GDP
    - `\(Y_4\)` Stock prices
]

.pull-right[
* $X = $ Political power 
    - `\(X_1\)` Presidents
    - `\(X_2\)` Governors
    - `\(X_3\)` Senators
    - `\(X_4\)` Representatives
]

* Model: regression line
`$$\mathbb{E}(Y_{k}) = \beta_0 + \beta_{j} X_{j}$$`
with null hypothesis 
`$$H_j: \beta_j = 0$$`

* Note that if you decide how to represent `\(Y\)` and `\(X\)` after seeing the data, the null hypothesis `\(H_j\)` is a random quantity because it is function of the data

---

layout: false
class: inverse, middle, center

# The law of selection

---

# Stock Tipsters

* From David Hand, The Improbability Principle

* How to (pretend to) *correctly predict, for ten weeks in a row, whether a stock will go up or down*

* If we assume that each stock has an equal probability of going up or down each week, then the probability of making such an accurate series of predictions by chance alone is just (1/2)^10  = 1/1024

* I'll begin by picking a stock — any stock will do. Then I'll choose 1024 innocent potential victims

* I'll send them a prediction about what the stock will do over the next week. For half of them I'll predict that the stock will go up, and for the other half I'll predict that it will go down. Half of my potential victims will receive the wrong prediction, and half the right one. 

* Now I'll forget about those who received the wrong prediction, and just focus on those who received the right prediction. For half of those, I'll now predict the stock will go up over the next week, and for the other half I’ll predict it will go down. And I’ll continue in this way, each time writing a new letter just to those who received the right prediction the previous week 

* At the end of the 10th week, I’m down to just one person. Everyone else at some point received an incorrect prediction, and I stopped sending them new letters. But what about this one person? They’ve seen me make ten correct predictions in a row. And this is the point at which I ask to pay for my next stock tip...

---

# Choosing someone for a job

* Imagine we want to choose the most able person for a job and aim to do this by testing the candidate

* We're going to choose the one candidate with the highest score.

* Now, test scores aren't perfect measures of ability (think of the exams you took in school: sometimes you did better than expected, sometimes worse, depending on exactly what questions were asked, how well you slept the night before, and so on)

* Suppose we have 20 candidates, all of the same ability, so that, on average, if we tested them lots of times, their average test scores would be the same, but 10 of whom produce much more variable scores than the others. For example, ten of the candidates might produce scores Uniform(45,55) while the other ten might produce scores Uniform(20,80). People in both sets have averages of 50, but those in the second set are much more variable

* It's clear that the one candidate with the highest score is much more likely to be from the second, more variable, set

* Downside: the candidates we are most likely to pick are also likely to have the most erratic performance on the job

---

# Regression to the mean

* Speed cameras are actually effective in reducing accidents?

* Suppose we have only 600 cameras, and have to decide at which of the 3600 sites to locate them

* Assume that the accidents occur purely by chance: the number of accidents at each location is determined by throwing a dice (from 1 to 6)

* Clearly we'll choose to position the cameras at (about) 600 sites  which show 6

* Now we follow developments over the next year to see what happens to the accident rate at each site where we've placed a camera

* The accident rate over this second year will be equivalent to the results of throwing for the second time the 600 dice that originally showed a 6

* The average of the second-year accident rate over all of the 600 locations will consequently be about 3.5

* It looks as if the accident rate has dramatically decreased. But the reduction has nothing to do with whether or not the cameras are effective. It’s just due to the law of selection, through regression to the mean

---

# Hindsight bias

* Consider 9/11. Why didn’t we see it coming? Documentaries about it show that signs were there from the beginning

* *After the fact* it’s easy to put the pieces together, and show how they form a continuous chain leading to the outcome

* *Before the fact*, however, there are many pieces and potential chains and it's just not possible to know which events fit together

* This isn't because there are too many pieces, but simply because they can be put together in a vast number of possible ways, and there’s no reason to select out any one of them


---

# P-values can be different than they appear

* Suppose you have p-values testing `\(m\)` truly null hypotheses `\(H_1,\ldots,H_m\)`

* Assume that all the hypotheses are true, i.e. all the p-values are Uniform(0,1)

* By selecting the minimum p-value `\(p^* = \min(p_1,\ldots,p_m)\)`, its distribution becomes
`$$\Pr(p^* \leq u) = 1 − (1 − u)^m$$`

* The same observed p-value of 
`$$p^*=0.015$$` should be read as 0.0298, 0.0443, 0.1403, or 0.7794 by adjusting for the effect of selection with `\(m\)` = 2, 3, 10, or 100


---

# Selection breaks Bonferroni

* Suppose we begin with `\(m\)` potential tests `\(X_j \sim N(\mu_j,1)\)` for `\(H_j: \mu_j = 0\)` vs `\(\bar{H}_j: \mu_j &gt; 0\)`

* Before we perform any tests, we first select only the ones that look
interesting

* For example, suppose that we select the `\(X_i\)` that have `\(X_i &gt; 1\)` and there are `\(s\)` of them 

* Now do Bonferroni with level `\(\alpha/s\)` instead of `\(\alpha/m\)`. Will Bonferroni control the FWER?

---


```r
selected_tests &lt;- function(m) {
X &lt;- rnorm(m)
XS &lt;- X[X &gt; 1]
s &lt;- length(XS)
rejections &lt;- sum(XS &gt; qnorm(1-.05/s))
FWE &lt;- as.integer(rejections &gt; 0)
return(FWE)
}
mean(replicate(1000, selected_tests(100)))
```

```
[1] 0.287
```

---

# The law of selection

&gt; The law of selection says you can make probabilities as high as you like if you choose after the event - David Hand, The Improbability Principle


Some other examples include:

* **Harking** (Hypothesizing After the Results are Known): Decide what hypothesis you are testing after you've carried out the experiment and collected the data

* **Publication bias** This is the tendency for scientific journals to preferentially publish studies which show a phenomenon rather than those which fail to show the phenomenon

* **Self-selected surveys** which are carried out by the popular press and on the Web: To produce reliable results, any survey should choose the sample of respondents carefully, so that any deductions based on it are likely to represent the overall views of the target population

* **Dropout bias** etc. 

---

layout: false
class: inverse, middle, center

# Confidence intervals for selected parameters

---

# Confidence intervals 


* So far, we have been studying multiple hypothesis testing

* Today we will look at the other inference
problem, namely, multiple confidence intervals and coverage after selection

* Consider a classical setting in which we have `\(m\)` parameters `\(\theta_1, \theta_2, \ldots, \theta_m\)` and corresponding estimators 
`$$T_i \sim N(\theta_i,1), \qquad i=1,\ldots,m$$`

* In this setting, we can construct `\(1-\alpha\)` confidence intervals
`$$\mathrm{CI}_i = [T_i - z_{1-\alpha/2}, T_i + z_{1-\alpha/2}], \qquad i=1,\ldots,m$$`

* Each interval has *marginal coverage*
`$$\Pr(\theta_i \in \mathrm{CI}_i ) = 1-\alpha, \qquad i=1,\ldots,m$$`

---

# Average coverage

*  Ignoring the *multiplicity* of confidence intervals (CIs) is generally more common than ignoring the problem of multiplicity in testing

* One reason why unadjusted CIs seem more acceptable than unadjusted tests is that they give the right coverage *on average*

* The proportion of `\(1-\alpha\)` CIs covering their respective parameters out of the intervals constructed is expected to be `\(1-\alpha\)`
`$$\mathbb{E}\left( \frac{1}{m} \sum_{i=1}^{m} I\{ \theta_i \in \mathrm{CI}_i \} \right) =  \frac{1}{m} \sum_{i=1}^{m} \Pr( \theta_i \in \mathrm{CI}_i )  = 1-\alpha$$`
 and thus only `\(\alpha\)` will not be covered. So why worry?

---

# Coverage after selection


* Often researchers examine many parameters at once and report confidence intervals for *selected ones*

* However, as noted by Soric in 1989:

&gt; In a large number of 95% confidence intervals, 95% of them contain the population
parameter [...] but it would be wrong to imagine that the same rule also applies to a
large number of 95% interesting confidence intervals

---

# Example

* Generate parameters `\(\theta_i\)` i.i.d. `\(N(0,0.04)\)`, `\(i=1,\ldots, 20\)`

* Assume `\(T_i \sim N(\theta_i,1)\)`, `\(i=1,\ldots, 20\)`

* Construct 90% confidence intervals `\(\mathrm{CI}_i = [T_i - 1.64, T_i + 1.64]\)`

* Select interesting parameters whose CIs do not cover 0

* Compute the coverage proportion of CIs for all parameters

* Compute the coverage proportion of CIs for selected parameters

---

&lt;img src="9_Selective_files/figure-html/unnamed-chunk-4-1.png" width="60%" style="display: block; margin: auto;" /&gt;

* As expected, 2 out of 20 CIs cover the respective parameters

* But only 1 out of 3 CIs covers the selected parameters (red)


---

# False coverage rate

* **False coverage rate** (FCR) is defined as
`$$\mathrm{FCR} = \mathbb{E}\left( \frac{\mathrm{number\,\,of\,\,selected\,\,CIs\,\,not \,\,covering}}{\mathrm{number\,\,of\,\,selected\,\,parameters}}\right)$$`

* It is similar to FDR in that it controls type I error over the selected parameters

* Without selection, the marginal CIs control the FCR

---

# Benjamini-Yekutieli (2005)

1. Apply the selection rule and select the parameters `\(\theta_i\)` with `\(i \in S\)`

2. FCR adjusted CIs are given by
`$$\widetilde{\mathrm{CI}}_i = [T_i - z_{1-\tilde{\alpha}/2}, T_i + z_{1-\tilde{\alpha}/2}], \qquad i \in S$$`
with 
`$$\tilde{\alpha} = \frac{(\#S) \alpha}{m}$$`

* If `\(T_i\)`'s are independent, then for any selection procedure, the FCR of adjusted CI's obey FCR `\(\leq \alpha\)`

* There also are extensions to PRDS statistics

---

&lt;img src="9_Selective_files/figure-html/unnamed-chunk-5-1.png" width="60%" style="display: block; margin: auto;" /&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
