---
title: "**Large-scale testing**"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true    
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, 
                      comment=NA, cache=F, R.options=list(width=220),
                      fig.align='center', out.width='60%', fig.asp=.7)
```


# Outline

* Heads or tails?

* Microarrays

* Large-scale testing

* Familywise error

* Bonferroni


---

# Checking whether a coin is fair

* A fair coin, when tossed, should have an equal chance of landing *head* H or *tail* T

* A biased coin is more likely to land heads than tails (or the other way around)

* How to check whether a coin is fair?

* Toss a coin 5 times

* Let $E$ the event TTTTT $\cup$ HHHHH, $\bar{E}$ any other result

* Tell me whether you have obtained $E$ or $\bar{E}$

* Now simulate 

```{r, eval=FALSE}
sample(c("T","C"),size=5,replace=T)
```

---

# One hypothesis

* Null hypothesis $H$: the coin is fair

* Alternative hypothesis $\bar{H}$: the coin is biased

* Statistical test: reject the null hypothesis $H$ if $E$ happens

* Type I error: reject the null hypothesis $H$ when it is true

* Probability of type I error
$$\alpha = \Pr_{fair}(E) = 2\cdot (1/2)^5 = 6.25\%$$

---

# Many hypotheses

* $i$th null hypothesis $H_i$: the $i$th coin is fair

* We are testing $m$ null hypothesis $H_1,\ldots,H_m$

* Let $F = \bigcup_{i=1}^{m}E_i$ the event "at least one event $E_i$ happened"

* If all the coins are fair, i.e. all null hypotheses are true, then the probability of committing *at least* one type I error is
$$\Pr_{all\,\,fair}(F) = 1 - [1-\Pr_{fair}(E)]^m = 1 - (0.945)^m$$

---

```{r}
m = 100
PrE = 2*(0.5)^5
PrF = 1 - (1-PrE)^(1:m)
plot(1:m, PrF, type="h", ylim=c(0,1))
```


---

# Mean and variance of the number of type I errors

* Assume that all the $m$ coins are fair

* $V = \sum_{i=1}^{m} 1\{E_i\}$ number of type I errors

* Because $E_1,\ldots,E_m$ are i.i.d. $\mathrm{Bernulli}(\Pr(E))$, then 
$$V \sim \mathrm{Binomial}(m,\Pr(E))$$

* $\mathbb{E}(V) = m \Pr(E)$

* $\mathbb{V}\mathrm{ar}(V) = m \Pr(E)[1-\Pr(E)]$

---

```{r}
m = 100
PrE = 2*(0.5)^5
alpha = 0.1
IC = sapply(1:m, function(i)
            qbinom(c(alpha/2,1-(alpha/2)), size=i, prob=PrE)
)
plot(1:m,(1:m)*PrE)
for (i in 1:m) segments(x0=i,x1=i,y0=IC[1,i],y1=IC[2,i])
```

---

# The three eras of statistics

From Efron (2012) [Large-Scale Inference](https://statweb.stanford.edu/~ckirby/brad/LSI/monograph_CUP.pdf)

1. **The age of huge census-level data sets** were brought to bear on simple but important questions: 
  - Are there more male than female births? 
  - Is the rate of insanity rising?

2. **The classical period** of Pearson, Fisher, Neyman, Hotelling, and their successors, intellectual giants who developed a theory of optimal inference capable of wringing every drop of information out of a scientific experiment. The questions dealt with still tended to be simple
    - Is treatment A better than treatment B?

3. **The era of scientific mass production**, in which new technologies typified by the *microarray* allow a single team of scientists to produce *high-dimensional data*. But now the flood of data is accompanied by a deluge of questions, perhaps thousands of estimates or hypothesis tests that the statistician is charged with answering together


---

# The four eras of data

From Jeff Leek's [post](https://simplystatistics.org/2016/12/16/the-four-eras-of-data/)

1. **The era of not much data** Prior to about 1995, usually we could collect a few measurements at a time. The whole point of statistics was to try to optimaly squeeze information out of a small number of samples - so you see methods like *maximum likelihood* and *minimum variance unbiased estimators* being developed

2. **The era of lots of measurements on a few samples** This one hit hard in biology with the development of the microarray and the ability to measure thousands of genes simultaneously. This is the same statistical problem as in the previous era but with a lot more noise added. Here you see the development of methods for *multiple testing* and *regularized regression* to separate signals from piles of noise

3. **The era of a few measurements on lots of samples** This era is overlapping to some extent with the previous one. Large scale collections of data from EMRs and Medicare are examples where you have a huge number of people (samples) but a relatively modest number of variables measured. Here there is a big focus on statistical methods for knowing how to model different parts of the data with *hierarchical models* and separating signals of varying strength with *model calibration*

4. **The era of all the data on everything** This is an era that currently we as civilians don’t get to participate in. But Facebook, Google, Amazon, the NSA and other organizations have thousands or millions of measurements on hundreds of millions of people. Other than just sheer computing I'm speculating that a lot of the problem is in segmentation (like in era 3) coupled with avoiding crazy overfitting (like in era 2)

---

# Microarrays

* For many statisticians, microarrays provided an introduction to *large-scale data analysis*. These were revolutionary biomedical devices that enabled
the assessment of individual activity for thousands of genes at once

* Need to carry out thousands of simultaneous
hypothesis tests, done with the prospect of finding only a few interesting genes among a haystack of null cases

---

# Microarrays

![](https://upload.wikimedia.org/wikipedia/commons/0/0e/Microarray2.gif)

---

# Prostate cancer data

* The prostate cancer data came from a microarray study of
$n=102$ men, 52 prostate cancer patients and 50 normal controls

* Each man’s gene expression levels were measured on a panel of $m=6033$
genes, yielding a $102 \times 6033$ matrix of measurements 
$$x_{ji}=\mathrm{activity\,\,of\,\,}i\mathrm{th\,gene\,\,for\,\,}j\mathrm{th\,\,subject}$$



* The $i$th null hypothesis, denoted $H_i$, would state that the mean expression level of the $i$th gene is the same in both groups of patients
$$H_i: \mathbb{E}(X_{i}^{cancer}) = \mathbb{E}(X_{i}^{control})$$

* For each gene, a two-sample t statistic $t_i$ is computed comparing
gene $i$ expression levels for the 52 patients with those for the 50
controls

* Under the null hypothesis $H_i$ that the patients’ and the controls’
responses come from the same normal distribution of gene $i$ expression
levels, $t_i$ will follow a standard Student t distribution with 100 degrees of
freedom


---

* `prostmat.csv` is the 6033x102 data matrix discussed at the top of page 33 of CASI

* The first 50 columns are the genetic activity measurements for the 50 control subjects,
while the last 52 columns represent the prostate cancer subjects. The data can be read directly into R via the command

```{r, eval=FALSE}
prostmat <- read.csv("http://web.stanford.edu/~hastie/CASI_files/DATA/prostmat.csv")
```

* Two-sample Student t test for the 50th gene

```{r, eval=FALSE}
i = 50
boxplot(t(prostmat)[1:50,i], t(prostmat)[-c(1:50),i])
t.test(x=t(prostmat)[1:50,i], y=t(prostmat)[-c(1:50),i], var.equal=T)
```


* `prostz.txt` is the vector of 6033 z-values pictured in Figure 3.4 of CASI

```{r, eval=FALSE}
prostz <- read.csv("http://web.stanford.edu/~hastie/CASI_files/DATA/prostz.txt", header=F)
```

---

```{r, echo=FALSE}
zscores <- read.csv("http://web.stanford.edu/~hastie/CASI_files/DATA/prostz.txt", header=F)$V1
pvalues <- 2*pnorm(abs(zscores), lower.tail = FALSE)
hist(pvalues, 20, freq=FALSE, main="", xlab="p-values")
abline(h=1,col=2)
```

Histogram of 6033 p-values, one for each gene
in the prostate cancer study. If all genes were null the
histogram would be uniform (red line). For which genes can we
reject the null hypothesis?

---

# Z scores

* The transformation
$$z_i = \Phi^{-1}(F_{100}(t_i))$$
where $F_{100}$ is the cdf of a $t_{100}$ distribution and $\Phi^{-1}$ the inverse function of
a standard normal cdf, makes $z_i$ standard normal under the null hypothesis
$$H_i: z_i \sim N(0,1)$$

* A reasonable model for both null and non-null genes is
$$z_i \sim N(\mu_i,1)$$
$\mu_i$ being the *effect size* for gene $i$

* Null genes have $\mu_i=0$, while the
investigators hoped to find non-null genes with large positive or negative $\mu_i$ effects, for which the patients and controls respond differently

---

```{r, echo=FALSE}
hist(zscores, 20, freq=FALSE, main="", xlab="z-values")
curve(dnorm, from=min(zscores), to=max(zscores), add=TRUE, col=2, lwd=2)
```

**Figure 15.1 of CASI** Histogram of 6033 z-values, one for each gene
in the prostate cancer study. If all genes were null the
histogram would track the red curve. For which genes can we
reject the null hypothesis?

---

# Large-scale testing

* Large-scale testing refers exactly to this situation: having observed a large number $m$ of test statistics, how should we decide which if any of the null hypotheses to reject? 

* Classical testing theory involved only a single
case, $m=1$

* A theory of multiple testing arose in the 1960s, "multiple" meaning $m$ between 2 and perhaps 20

* The microarray era produced data
sets with $m$ in the hundreds, thousands, and now even millions

* The most troubling fact about large-scale testing is how easy it is to be
fooled

---

# Errors 

* Hypothesis tests are not free of error, however, and for every hypothesis test there is a risk of falsely rejecting a hypothesis that is true, i.e. a *type I error*, and of failing to reject a hypothesis that is false, i.e. a *type II error*

* In hypothesis testing, type I errors are traditionally considered *more problematic* than type II errors

* If a rejected hypothesis allows publication of a scientific finding, a type I error brings a *false discovery*, and the risk of publication of a potentially misleading scientific result

* Type II errors, on the other hand, mean missing out on a scientific result. Although unfortunate for the individual researcher, the latter is, in comparison, less harmful to scientific research as a whole

* In hypothesis testing the probability of making a type I error is bounded by $\alpha$, an acceptable risk of type I errors, conventionally set at 0.05. 
Problems arise, however, when researchers do not perform a single hypothesis test but *many of them*

* Since each test has a probability of producing a type I error, performing a large number of hypothesis tests virtually guarantees the presence of type I errors among the findings. Actually, the expected number of type I errors is 

$$\mathbb{E}(\mathrm{number\,\,of\,\,type\,\,I\,\,errors})=\mathrm{number\,\,of\,\,true\,\,null\,\,hypotheses}\times \alpha$$

---

# Multiple testing

* We have a collection $\mathcal{H}=\{H_1, \ldots, H_m\}$ of null hypotheses, which we would like to reject

* An unknown number $m_0$ of these hypotheses is true, whereas the other $m_1 = m-m_0$ is false. We call the collection of true hypotheses $\mathcal{T} \subseteq \mathcal{H}$ and the remaining collection of false hypotheses $\mathcal{F} = \mathcal{H} \setminus \mathcal{T}$. We denote the proportion of true hypotheses $\pi_0 = m_0/m$.

* The goal of a *multiple testing procedure* is to choose a collection $\mathcal{R} \subseteq \mathcal{H}$ of hypotheses to reject 

* If we have $p$-values $p_1,\ldots,p_m$ for each of the hypotheses $H_1,\ldots,H_m$, and obvious choice is the collection 
$$\mathcal{R} = \{H_i: p_{i}\leq T\}$$
rejecting all hypotheses with a $p$-value below a threshold $T$

* In this situation, the multiple testing problem reduces to the choice of $T$

* Ideally, the set of rejected hypotheses $\mathcal{R}$ should coincide with the set $\mathcal{F}$ as much as possible

---

# Summary of multiple testing

* Two types of error can be made: false positives, or type I errors, are the rejected hypotheses that are not false, i.e. $\mathcal{R} \cap \mathcal{T}$; false negatives or type II errors are the false hypotheses that we failed to reject, i.e. $\mathcal{F} \setminus \mathcal{R}$

* Rejected hypotheses are sometimes called *discoveries*, hence the terms *true discovery* and *false discovery* are sometimes used for correct and incorrect rejections

* We can summarize the numbers of errors occurring in a hypothesis testing procedure in a contingency table. We can observe $m$ and $R = \#\mathcal{R}$, but all quantities in the first two columns of the table are unobservable

|   | true  | false  | total  | 
|---|---|---|---|
| rejected  | $V$  | $U$  |  $R$ |
| not rejected   | $m_0-V$  |  $m_1-U$  | $m-R$   |
| total  | $m_0$  | $m_1$  | $m$  |

* Type I and type II errors are in direct competition with each other, and a trade-off between the two must be made. If we reject more hypotheses, we typically have more type I errors but fewer type II errors

---

* $Z_i \sim N(\mu_i,1)$ is the test statistic

* $H_i: \mu_i = 0$ vs $\bar{H}_i: \mu_i > 0$

* Reject $H_i$ if $Z_i \geq z_{1-\alpha}$ or equivalently, when $p_i = 1-\Phi(Z_i) \leq \alpha$

where $z_{1-\alpha}$ is the $1-\alpha$ quantile of $N(0,1)$ and $\Phi$ is the cdf of $N(0,1)$


```{r}
set.seed(123)
alpha = 0.05
m = 100
m0 = 80
effect = 3
setT = sample(1:m, size=m0, replace=F)
stats <- rnorm(m)
stats[-setT] <- stats[-setT] + effect
pvals = pnorm(stats, lower.tail = FALSE)
setR = which(pvals <= alpha)
# setR = which(stats >= qnorm(1-alpha)) # equivalently
table( rejected= 1:m %in% setR,
true = 1:m %in% setT)
```

---

# Familywise error rate

* Multiple testing methods try to reject as many hypotheses as possible while keeping some measure of type I errors in check 

* The most classical way of controlling for multiple testing is by **familywise error rate** (FWER) control

* The FWER is the probability that the rejected set contains any error:
$$
\mathrm{FWER} = \mathrm{P}(V > 0) 
$$

* The FWER is *controlled* at level $\alpha$ when the set $\mathcal{R}$ (i.e. the threshold $T$) is chosen in such a way that 
$$\mathrm{FWER}  \leq \alpha$$

---

# Bonferroni

* The method of Bonferroni controls FWER at level $\alpha$ by rejecting null hypotheses only if they have $p$-value
smaller than $\alpha/m$:

$$\mathcal{R} = \left\{H_i: p_{i}\leq \frac{\alpha}{m}\right\}$$

* The **adjusted** $p$-value for the Bonferroni procedure is given by $$\tilde{p}_i = \min(mp_i, 1)$$
and the Bonferroni method rejects null hypotheses if they have adjusted $p$-value
smaller than $\alpha$:
$$\mathcal{R} = \{H_i: \tilde{p}_{i}\leq \alpha \}$$

* Bonferroni is the simplest, oldest and most well-known multiple testing method, and is attractive because of its simplicity

* The FWER control of Bonferroni is valid for *all* dependence structures of the underlying $p$-values

* However, it is also known to be conservative, especially if many hypotheses are false, or if strong positive correlations
between $p$-values occur

---

# Proof

* Assumptions on the $p$-values often involve only the
$p$-values of true hypotheses. We denote these by $q_1, \ldots, q_{m_0}$. By the definition of a $p$-value, if their
corresponding hypotheses are true, these $p$-values are either uniformly distributed between 0 and 1, or
they can be stochastically greater than uniform, i.e. 
$$
\mathrm{Pr}(q_i \leq u ) \leq u
$$

* To properly motivate the Bonferroni method, we should look at it as a corollary to Boole’s inequality,
which says that for any collection of events $E_1, \ldots, E_k$, we have
$$\mathrm{P}\big(\bigcup_{i=1}^k E_i\big) \leq \sum_{i=1}^k \mathrm{P}(E_i)$$


* It follows from Boole's inequality that the probability that there is some $i$ for which $q_i \leq \alpha/m$ is given by
$$\Pr\big(\bigcup_{i=1}^{m_0} \{ q_i \leq \alpha/m \}\big) \leq \sum_{i=1}^{m_0} \mathrm{P}( q_i \leq \alpha/m ) \leq m_0\frac{\alpha}{m} \leq \alpha$$

* Because the method of Bonferroni only commits a type I error if $q_i \leq \alpha/m$ for some $i$, this proves
FWER control at level $\alpha$ for the Bonferroni method


---

```{r}
set.seed(123)
alpha = 0.05
m = 100
m0 = 80
effect = 3
setT = sample(1:m, size=m0, replace=F)
stats <- rnorm(m)
stats[-setT] <- stats[-setT] + effect
pvals = pnorm(stats, lower.tail = FALSE)
setR = which(pvals <= alpha/m)
# setR = which(stats >= qnorm(1-alpha/m)) # equivalently
table( rejected= 1:m %in% setR,
true = 1:m %in% setT)
```

---


# Magnitude of Bonferroni's threshold

* $Z_i \sim N(\mu_i,1)$ is the test statistic

* $H_i: \mu_i = 0$ vs $\bar{H}_i: \mu_i > 0$

* Bonferroni rejects $H_i$ if $Z_i \geq z_{1-\alpha/m}$ 

* Holding $\alpha$ fixed, we can show that for large $m$
$$z_{1-\alpha/m} \approx \sqrt{2\log m}$$

* One remarkable fact about all of this is that there is asymptotically no dependence on $\alpha$

---

```{r}
alpha = 0.05
m = 10^(2:12)
threshold = qnorm(1-alpha/m)
approx = sqrt(2*log(m))
plot(m,threshold,type="l",log="x")
lines(m,approx, col=2)
```

---

# Holm procedure

* Begin by ordering the p-values in ascending order
$$p_{(1)}\leq p_{(2)}\leq \ldots \leq p_{(m)}$$
and let $H_{(1)}, H_{(2)},\ldots, H_{(m)}$ be the corresponding hypotheses 

* Step 1: If $p_{(1)}\leq \alpha/m$ reject $H_{(1)}$ and go to Step 2.
Stop otherwise

* Step 2: If $p_{(2)}\leq \alpha/(m-1)$ reject $H_{(2)}$ and go to Step 3.
Stop otherwise

* $\ldots$

* Step $i$: If $p_{(i)}\leq \alpha/(m-i+1)$ reject $H_{(i)}$ and go to Step $i+1$. Stop otherwise

* $\ldots$

* Step $m$: If $p_{(m)}\leq \alpha$ reject $H_{(m)}$

---

# Holm > Bonferroni

* Holm compares each $p$-value $p_{(i)}$ to its corresponding critical value $\alpha/(m-i+1)$

* Holm's method finds the smallest $j$ such that $p_{(j)}$ exceeds $\alpha/(m-j+1)$, and subsequently rejects all $j-1$ hypotheses with a $p$-value at most $\alpha/(m-j)$. If no such $j$ can be found, all hypotheses are rejected

* Holm's method is a sequential variant of the Bonferroni method that always rejects at least as much as Bonferroni's method, and often a bit more, but still has valid FWER control under the same assumptions

* From this perspective, there is no reason, aside from possibly simplicity, to even use Bonferroni's method in preference to Holm's.

---

# Independent p-values

* The FWER control of Bonferroni is valid for *all* dependence structures of the underlying $p$-values

* If we assume that the null $p$-values $q_1,\ldots,q_{m_0}$ are i.i.d. Uniform(0,1), then the FWER for $\mathcal{R} = \left\{H_i: p_{i}\leq t \right\}$ is
$$\mathrm{FWER} = \Pr\big(\bigcup_{i=1}^{m_0} \{ q_i \leq t \}\big) = 1-\Pr\big(\bigcap_{i=1}^{m_0} \{ q_i > t \}\big) = 1-(1-t)^{m_0}$$

* Under the independence assumption, we can use the **Sidak** critical value
$$\mathcal{R} = \left\{H_i: p_{i}\leq 1-(1-\alpha)^{1/m} \right\}$$
and control the FWER at $\alpha$

* For large $m$, the ratio between Sidak and Bonferroni critical values is
$$\frac{1-(1-\alpha)^{1/m}}{\alpha/m} \approx \frac{-\log(1-\alpha)}{\alpha}$$
which evaluates to only 1.026 for $\alpha=0.05$