---
title: "Course overview"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true    
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, 
                      comment=NA, cache=F, R.options=list(width=220),
                      fig.align='center', out.width='60%', fig.asp=.7)
```


# Outline

* Course overview
* Algorithms and inference
* Variable selection
* Jelly beans cause acne?
* Which party is correlated with economic success?

---
layout: false
class: inverse, middle, center

# Course overview

---

# Teacher

Aldo Solari

**E-mail** aldo.solari@unimib.it

**Web page** https://aldosolari.github.io/

**Office hours** Thursday, 17:30-18:30, room 2030, building U7, II floor

**Course page** https://aldosolari.github.io/SL/

| When | Where | Hours |
|---|---|----|
| Monday | Lab713 | 13:30-16:30 |
| Wednesday | Lab713 | 15:30-18:30 |
| Thursday | Lab713 | 12:30-14:30 |

.center[**Time table**]

---

# The course

* __Big Data__ and __Data Science__ have now become recurring terms in media communication. In response to the growing need to analyze data, the profession of the __data scientist__ has recently emerged

* This course aims to provide __new tools__ for the data scientist

* These tools require __technical skills__ such as 
    - modern statistical methods
    - advanced programming
    
* They also require __professional skills__ such as 
    - communication
    - teamwork
    - problem solving
    - critical thinking

---

# Topics

The topics covered by this course are

* __Advanced statistical/machine learning models__
    - Generalized Additive Models
    - Support Vector Machines
    - Boosting algorithms
    - Ensemble learning
    
* __Large-scale testing__ & __selective inference__

* __High-dimensional inference__
    - sample-splitting inference
    - stability selection
    - knockoffs


---

# Exam

* The exam consists of two parts:

    - __Written exam__ (open-ended and/or closed-ended questions)

    - __Oral exam__, including the __presentation__ of a data science __project__ (homework)
        - Attending students can form a __team__ (max. 3 persons) and present at the end of the course. In this case the oral exam is optional

* The final grade is a weighted average of written exam (1/2) and oral exam (1/2)

* See the [course syllabus](https://aldosolari.github.io/SL/syllabus/syllabus.html) for further informations

---

# Project


* Your final project is to do a __novel data analysis__ to answer a __question__ and write a __blog post__ about it on github

* The blog post should answer:
    1. __What is the question(s) you tried to answer? Why should someone care?__
    
    2. What is the data/how did you get it?
    
    3. How did you answer the questions (e.g. what statistical techniques, etc)?
    
    4. __What are your findings?__

---

# An example

* __Project__: [fitteR happieR](https://www.rcharlie.com/img/posts/fitter-happier/album_chart.html) by [RCharlie](https://www.rcharlie.com/)

* __Question__: which is the most depressing Radiohead song?

* __Data__ : 
    - Spotify API and the R package *spotifyr* to get the *valence* metric (the song's positivity)
    - Genius Lyrics API and the R package *rvest* to get the *lyrics* (text data)

* __Statistics__ : 
    - Sentiment analysis on lyrics to get the % of sad words in a lyric 
    - $$GloomIndex = \frac{(1-valence)+pctSad*(1+lyricalDensity)}{2}$$

* __Visualization__ : https://www.rcharlie.com/img/posts/fitter-happier/album_chart.html

* __Answer__: True Love Waits 

---

# An example

* __Project__: [fitteR happieR](https://www.rcharlie.com/img/posts/fitter-happier/album_chart.html) by [RCharlie](https://www.rcharlie.com/)

* __Question__: which is the most depressing Radiohead song?

* __Data__ : 
    - Spotify API and the R package *spotifyr* to get the *valence* metric (the song's positivity)
    - Genius Lyrics API and the R package *rvest* to get the *lyrics* (text data)

* __Statistics__ : 
    - Sentiment analysis on lyrics to get the % of sad words in a lyric 
    - $$GloomIndex = \frac{(1-valence)+pctSad*(1+lyricalDensity)}{2}$$

* __Visualization__ : https://www.rcharlie.com/img/posts/fitter-happier/album_chart.html

* __Answer__: True Love Waits 

---

layout: false
class: inverse, middle, center

# Algorithms and inference

---

# Algorithms and inference

Statistics is the science of *learning from experience*:

* the successes and failures of a new
experimental drug

* the uncertain measurements of an asteroid’s path toward
Earth

* Etc.

There are two main aspects of statistical analysis:

* the *algorithmic* aspect
* the *inferential* aspect

---

# Averaging

* The distinction begins with the
most basic, and most popular, statistical method, __averaging__

* Suppose we have observed numbers $y_1,y_2,\ldots,y_n$ applying to some phenomenon of
interest, perhaps the automobile accident rates in the $n=50$ states

* We may assume that $y_1,\ldots,y_n$ are i.i.d. realizations of the random variable $Y$, and 
suppose that our __parameter of interest__ is $\mathbb{E}(Y)$.

* Averaging is the __algorithm__
$$\bar{y} = \frac{1}{n}\sum_{i=1}^{n}y_i$$
How accurate is that number?

---

# Uncertainty

* The standard error provides an __inference__ on the algorithm's accuracy 
$$\widehat{\mathrm{se}} = \sqrt{\frac{1}{n} \frac{\sum_{i=1}^{n}(y_i - \bar{y})^2}{n-1}}$$

* Of course, $\widehat{\mathrm{se}}$ is itself an algorithm, which could be (and is) subject
to further inferential analysis concerning its accuracy

* The point is that
the algorithm comes __first__ and the inference follows at a __second__ level of
statistical consideration

---

# Kidney data


```{r}
kidney <- read.table("https://web.stanford.edu/~hastie/CASI_files/DATA/kidney.txt", header=TRUE, sep=" ")
plot(kidney)
```

---

# Question of interest

* Data points $(x_i,y_i)$ have
been observed for $n=157$ healthy volunteers, with $x_i$ the $i$th volunteer’s
age in years, and $y_i$ a composite measure of overall kidney function

* Kidney function generally declines with age, as evident in the downward scatter
of the points

* The rate of decline is an important question in kidney
transplantation: in the past, potential donors past age 60 were prohibited,
though, given a shortage of donors, this is no longer enforced

* A potential new donor, aged 65, has appeared, and we wish
to assess his kidney fitness without subjecting him to an arduous series of
medical tests

* The new donor has $(x^*,y^*)=(65,?)$. Suppose that in kidney transplantation we need a value > -4.  Is $y^* > -4$ with high probability?

---

# Regression analysis of the kidney data 

Table 1.1 of CASI: regression line $y = \hat{\beta}_0 + \hat{\beta}_1 x$ estimates of kidney fitness and their standard errors for ages = $20,30,40,50,60,70,80$

```{r}
fit <- lm(tot ~ age, data=kidney)
ages <- data.frame(age = seq(20, 80, 10))
res <- predict(fit, newdata=ages, se.fit = TRUE)
knitr::kable(data.frame(age=ages$age, estimate=res$fit,se=res$se.fit), format="html" )
```

---

Figure 1.1 of CASI: Kidney fitness vs age for 157 volunteers. The
line is a linear regression fit, showing $\pm 2$ standard errors at
selected values of age

```{r}
plot(kidney)
abline(fit, col="darkgreen")
for (i in 1:nrow(ages)){
segments(x0=ages$age[i], y0=res$fit[i]-2*res$se.fit[i], y1=res$fit[i]+2*res$se.fit[i], col="darkgreen")
}
```

---

Figure 1.2 of CASI: Local polynomial fit to the
kidney-fitness data

```{r}
plot(kidney)
lines(lowess(kidney, f = 1/3), col="darkgreen")
```

---

Regression splines by using natural cubic spline with 8 degrees of freedom


```{r,  message=FALSE}
require(splines)
fit = lm(tot ~ ns(age,8), kidney)
plot(kidney)
lines(kidney$age, fitted(fit), pch="x",col=2)
```

---

# Confidence and prediction intervals

90% confidence interval for $\mathbb{E}(Y^*|x^*=65)$ 

```{r}
require(splines)
predict(fit, newdata=data.frame(age=65), interval = "confidence", level=.9)
```

90% prediction interval for $Y^*|x^*=65$ 

```{r}
require(splines)
predict(fit, newdata=data.frame(age=65), interval = "prediction", level=.9)
```


---

layout: false
class: inverse, middle, center

# Variable selection

---

# Simulated data

I've generated data from the Gaussian linear model
$$ y = N(1_n \beta_0 + X\beta, \sigma^2I_n)$$
where 

* $\underset{n\times 1}{y} = (y_1,\ldots,y_n)'$ is the response on $n$ observations

* $\underset{n\times p}{X}$ is the design matrix containing the measurements on $p$ variables 

* $\underset{p\times 1}{\beta} = (\beta_1,\ldots,\beta_p)'$ is the vector of coefficients of interest

* $\beta_0$ and $\sigma^2$ are nuisance parameters

* $\underset{n\times 1}{1_n} = (1,1,\ldots,1)'$ is a vector of ones of length $n$ and $\underset{n\times n}{I_n}$ is the identity matrix

---

# Variable selection 

The dataset `vs.Rdata` has $n=50$ and $p=31$

* The goal is select the relevant variables, i.e. to estimate
$$S = \{j \in \{1,\ldots,p\}: \beta_j \neq 0\}$$