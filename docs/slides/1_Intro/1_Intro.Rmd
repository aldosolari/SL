---
title: "Course overview"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true    
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, 
                      comment=NA, cache=F, R.options=list(width=220),
                      fig.align='center', out.width='60%', fig.asp=.7)
```


# Outline

* Course overview
* Algorithms and inference
* Variable selection
* Jelly beans cause acne?
* Which party is correlated with economic success?

---
layout: false
class: inverse, middle, center

# Course overview

---

# Teacher

Aldo Solari

**E-mail** aldo.solari@unimib.it

**Web page** https://aldosolari.github.io/

**Office hours** Thursday, 17:30-18:30, room 2030, building U7, II floor

**Course page** https://aldosolari.github.io/SL/

| When | Where | Hours |
|---|---|----|
| Monday | Lab713 | 13:30-16:30 |
| Wednesday | Lab713 | 15:30-18:30 |
| Thursday | Lab713 | 12:30-14:30 |

.center[**Time table**]

---

# The course

* __Big Data__ and __Data Science__ have now become recurring terms in media communication. In response to the growing need to analyze data, the profession of the __data scientist__ has recently emerged

* This course aims to provide __new tools__ for the data scientist

* These tools require __technical skills__ such as 
    - modern statistical methods
    - advanced programming
    
* They also require __professional skills__ such as 
    - communication
    - teamwork
    - problem solving
    - critical thinking

---

# Topics

The topics covered by this course are

* __Advanced statistical/machine learning models__
    - Generalized Additive Models
    - Support Vector Machines
    - Boosting algorithms
    - Ensemble learning
    
* __Large-scale testing__ & __selective inference__

* __High-dimensional inference__
    - sample-splitting inference
    - stability selection
    - knockoffs


---

# Exam

* The exam consists of two parts:

    - __Written exam__ (open-ended and/or closed-ended questions)

    - __Oral exam__, including the __presentation__ of a data science __project__ (homework)
        - Attending students can form a __team__ (max. 3 persons) and present at the end of the course. In this case the oral exam is optional

* The final grade is a weighted average of written exam (1/2) and oral exam (1/2)

* See the [course syllabus](https://aldosolari.github.io/SL/syllabus/syllabus.html) for further informations

---

# Project


* Your final project is to do a __novel data analysis__ to answer a __question__ and write a __blog post__ about it on github

* The blog post should answer:
    1. __What is the question(s) you tried to answer? Why should someone care?__
    
    2. What is the data/how did you get it?
    
    3. How did you answer the questions (e.g. what statistical techniques, etc)?
    
    4. __What are your findings?__

---

# An example

* __Project__: [fitteR happieR](https://www.rcharlie.com/img/posts/fitter-happier/album_chart.html) by [RCharlie](https://www.rcharlie.com/)

* __Question__: which is the most depressing Radiohead song?

* __Data__ : 
    - Spotify API and the R package *spotifyr* to get the *valence* metric (the song's positivity)
    - Genius Lyrics API and the R package *rvest* to get the *lyrics* (text data)

* __Statistics__ : 
    - Sentiment analysis on lyrics to get the % of sad words in a lyric 
    - $$GloomIndex = \frac{(1-valence)+pctSad*(1+lyricalDensity)}{2}$$

* __Visualization__ : https://www.rcharlie.com/img/posts/fitter-happier/album_chart.html

* __Answer__: True Love Waits 

---

# An example

* __Project__: [fitteR happieR](https://www.rcharlie.com/img/posts/fitter-happier/album_chart.html) by [RCharlie](https://www.rcharlie.com/)

* __Question__: which is the most depressing Radiohead song?

* __Data__ : 
    - Spotify API and the R package *spotifyr* to get the *valence* metric (the song's positivity)
    - Genius Lyrics API and the R package *rvest* to get the *lyrics* (text data)

* __Statistics__ : 
    - Sentiment analysis on lyrics to get the % of sad words in a lyric 
    - $$GloomIndex = \frac{(1-valence)+pctSad*(1+lyricalDensity)}{2}$$

* __Visualization__ : https://www.rcharlie.com/img/posts/fitter-happier/album_chart.html

* __Answer__: True Love Waits 

---

layout: false
class: inverse, middle, center

# Algorithms and inference

---

# Algorithms and inference

Statistics is the science of *learning from experience*:

* the successes and failures of a new
experimental drug

* the uncertain measurements of an asteroid’s path toward
Earth

* Etc.

There are two main aspects of statistical analysis:

* the *algorithmic* aspect
* the *inferential* aspect

---

# Averaging

* The distinction begins with the
most basic, and most popular, statistical method, __averaging__

* Suppose we have observed numbers $y_1,y_2,\ldots,y_n$ applying to some phenomenon of
interest, perhaps the automobile accident rates in the $n=50$ states

* We may assume that $y_1,\ldots,y_n$ are i.i.d. realizations of the random variable $Y$, and 
suppose that our __parameter of interest__ is $\mathbb{E}(Y)$.

* Averaging is the __algorithm__
$$\bar{y} = \frac{1}{n}\sum_{i=1}^{n}y_i$$
How accurate is that number?

---

# Uncertainty

* The standard error provides an __inference__ on the algorithm's accuracy 
$$\widehat{\mathrm{se}} = \sqrt{\frac{1}{n} \frac{\sum_{i=1}^{n}(y_i - \bar{y})^2}{n-1}}$$

* Of course, $\widehat{\mathrm{se}}$ is itself an algorithm, which could be (and is) subject
to further inferential analysis concerning its accuracy

* The point is that
the algorithm comes __first__ and the inference follows at a __second__ level of
statistical consideration

---

# Kidney data


```{r}
kidney <- read.table("https://web.stanford.edu/~hastie/CASI_files/DATA/kidney.txt", header=TRUE, sep=" ")
plot(kidney)
```

---

# Question of interest

* Data points $(x_i,y_i)$ have
been observed for $n=157$ healthy volunteers, with $x_i$ the $i$th volunteer’s
age in years, and $y_i$ a composite measure of overall kidney function

* Kidney function generally declines with age, as evident in the downward scatter
of the points

* The rate of decline is an important question in kidney
transplantation: in the past, potential donors past age 60 were prohibited,
though, given a shortage of donors, this is no longer enforced

* A potential new donor, aged 65, has appeared, and we wish
to assess his kidney fitness without subjecting him to an arduous series of
medical tests

* The new donor has $(x^*,y^*)=(65,?)$. Suppose that in kidney transplantation we need a value > -4.  Is $y^* > -4$ with high probability?

---

# Regression analysis of the kidney data 

Table 1.1 of CASI: regression line $y = \hat{\beta}_0 + \hat{\beta}_1 x$ estimates of kidney fitness and their standard errors for ages = $20,30,40,50,60,70,80$

```{r}
fit <- lm(tot ~ age, data=kidney)
ages <- data.frame(age = seq(20, 80, 10))
res <- predict(fit, newdata=ages, se.fit = TRUE)
knitr::kable(data.frame(age=ages$age, estimate=res$fit,se=res$se.fit), format="html" )
```

---

Figure 1.1 of CASI: Kidney fitness vs age for 157 volunteers. The
line is a linear regression fit, showing $\pm 2$ standard errors at
selected values of age

```{r}
plot(kidney)
abline(fit, col="darkgreen")
for (i in 1:nrow(ages)){
segments(x0=ages$age[i], y0=res$fit[i]-2*res$se.fit[i], y1=res$fit[i]+2*res$se.fit[i], col="darkgreen")
}
```

---

Figure 1.2 of CASI: Local polynomial fit to the
kidney-fitness data

```{r}
plot(kidney)
lines(lowess(kidney, f = 1/3), col="darkgreen")
```

---

Regression splines by using natural cubic spline with 8 degrees of freedom


```{r,  message=FALSE}
require(splines)
fit = lm(tot ~ ns(age,8), kidney)
plot(kidney)
lines(kidney$age, fitted(fit), pch="x",col=2)
```

---

# Confidence and prediction intervals

90% confidence interval for $\mathbb{E}(Y^*|x^*=65)$ 

```{r}
require(splines)
predict(fit, newdata=data.frame(age=65), interval = "confidence", level=.9)
```

90% prediction interval for $Y^*|x^*=65$ 

```{r}
require(splines)
predict(fit, newdata=data.frame(age=65), interval = "prediction", level=.9)
```


---

layout: false
class: inverse, middle, center

# Variable selection

---

# Simulated data

I've generated data from the Gaussian linear model
$$ y = N(1_n \beta_0 + X\beta, \sigma^2I_n)$$
where 

* $\underset{n\times 1}{y} = (y_1,\ldots,y_n)'$ is the response on $n$ observations

* $\underset{n\times p}{X}$ is the design matrix containing the measurements on $p$ explanatory variables 

* $\underset{p\times 1}{\beta} = (\beta_1,\ldots,\beta_p)'$ is the vector of coefficients of interest

* $\beta_0$ and $\sigma^2$ are nuisance parameters

* $\underset{n\times 1}{1_n} = (1,1,\ldots,1)'$ is a vector of ones of length $n$ and $\underset{n\times n}{I_n}$ is the identity matrix

---

# Variable selection 

* The dataset `vs.Rdata` has $n=50$ and $p=31$

```{r}
rm(list=ls())
library(repmis)
source_data("https://github.com/aldosolari/SL/blob/master/data/vs.Rdata?raw=true")
```

* The $j$th explanatory variable is __relevant__ in explaing the response if $\beta_j\neq 0$, and __irrelevant__  if $\beta_j= 0$

* The goal is select the relevant variables, i.e. to estimate
$$S = \{j \in \{1,\ldots,p\}: \beta_j \neq 0\}$$


* $\hat{S} \setminus S$ are __wrong selections__

* $S \setminus \hat{S}$ are __missed selections__

* What is your $\hat{S}$?

---

layout: false
class: inverse, middle, center

# Jelly beans cause acne?

---

# Jelly beans cause acne?

* Somebody claims that jelly beans cause acne

* The scientists investigate by designing a randomized experiment with $n = 10000$ subjects
    
-  The subjects are randomly allocated to the jelly beans group ( $n_1=5000$ )
 and to the placebo group ( $n_0=5000$ )
 
- At the end of the month, the presence of acne is recorded (1 if present, 0 otherwise)
 
* Load the dataset `jellybeans.Rdata`

```{r}
rm(list=ls())
library(repmis)
source_data("https://github.com/aldosolari/SL/blob/master/data/jellybeans.Rdata?raw=true")
```

---

# $2\times 2$ table

```{r}
n0 = nrow(jellybeans)/2
n1 = n0
y = sum(jellybeans$acne[jellybeans$treatment=="jellybean"])
s = sum(jellybeans$acne) 

tab = matrix(c(
  y, n0-y,
  s-y, n1-s+y
), ncol=2, byrow=TRUE)
colnames(tab) = c("acne","noacne")
rownames(tab) = c("jellybean","placebo")
addmargins(tab)
```

Jelly beans cause significantly more acne? 

---

# Fisher's exact test

* Fisher argued for carrying out the hypothesis test conditional
on the marginals of the table

* With the marginals fixed, the
number $Y$ in the upper left cell determines the other three cells by subtraction

|  | Acne | no acne | Sum |
|---|---|---|---|
| Jelly beans | $Y$ | $N_{1} - Y$  | $N_{1}$ |
| Placebo | $S-Y$ | $n_{0} - S + Y$ | $N_{0}$ |
| Sum   | $S$ | $N_{1} + N_{0}-S$ | $N_{1} + N_{0}$ |

* We need only test whether the observed number $y=889$ is too big under the null hypothesis of no treatment difference

* Under the null hypothesis, the random variable $Y|(S,N_{1},N_{0})$ follows the Hypergeometric Distribution with
$$\mathrm{Pr}(Y=y|S,N_{1},N_{0}) =  \frac{ {N_{0} \choose S-Y} {N_{1} \choose Y} }{ {N_{1} + N_{0} \choose S} }$$


---

```{r}
plot(0:s,dhyper(0:s, n0, n1, s), type="h", xlab="y", ylab="Probability")
points(y,0,col=2,pch=19)
fisher.test(tab, alternative="greater")$p.value
```

---

# Color?

* We found no link between jelly beans and acne ( $p>0.05$ )

* But somebody claims that it's only a certain __color__ that causes it

* Jelly beans have 20 colors: purple, brown, pink, blue, teal, salmon, green, turquoise, magenta, yellow, grey, tan, cyan, lilac, mauve, beige, red, black, peach, orange

* Purple jelly beans cause significantly more acne? Brown jelly beans cause significantly more acne? Pink jelly beans cause significantly more acne? Etc.


---

layout: false
class: inverse, middle, center

# Which party is correlated with economic success?

---

# Connection between politics and economy

* You're a social scientist with a question: is the U.S. economy affected by whether Republicans or Democrats are in office?

* Try to show that a connection exists, using real data going back to 1948

* For your results to be publishable in an academic journal, you'll need to prove that they are statistically significant by achieving a low enough $p$-value

* The more democratic power, the better the economy? Or the other way around?

---

# Select

* How do you want to measure economic performance?
    - Employment
    - Inflation
    - GDP
    - Stock prices
    
* Which politicians do you want to include?
    - President 
    - Governors
    - Senators
    - Representatives

* Other options
    - Factor in power (weight more powerful positions more heavily)
    - Exclude recessions (don't include economic recessions)
