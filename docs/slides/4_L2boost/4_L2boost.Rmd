---
title: "**Boosting with squared error loss**"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true    
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, 
                      comment=NA, cache=F, R.options=list(width=220),
                      fig.align='center', out.width='60%', fig.asp=.7)
```


# Outline

* Boosting

* L2-boosting

---

# Boosting

* Boosting is a general method for building a complex
prediction model using simple building components (called __weak learners__)

* Boosting was originally proposed (algorithm __Adaboost__) as a means for improving the performance of decision trees in binary classification problems


* Boosting starts by fitting a weak learner to the training data

* Next the weak learner is re-fitted, but with __more
weight__ (importance) given to badly fitted/misclassified observations

* This process is repeated until some stopping rule is reached

---

# Boosting algorithms

We will discuss the following algorithms:

*  __boosting with squared error loss__ (L2-boosting) by using regression trees as weak learners

*  __boosting with exponential loss__ (AdaBoost.M1) by using classification trees as weak learners

*  __gradient boosting__ (general algorithm)


L2-boosting with regression trees builds a model by repeatedly fitting a regression tree to the __residuals__. This is different from random forests because each tree is trying to amend errors
made by the ensemble of previously grown trees

---

# L2-boosting algorithm for regression trees

* [1.] Initialize $\hat{f}(x)= \bar{y}$ and $r_i = y_i - \bar{y}$ for $i=1,\ldots,n$ 

* [2.] For $b=1,2,\ldots, B$, repeat:

    - [(a)] Fit a tree $\hat{f}^b$ with $d$ splits to the data $(x_1,r_1),\ldots, (x_n,r_n)$
    - [(b)] Update $\hat{f}$ by adding in a shrunken version of the new tree:
$$\hat{f}(x) \leftarrow \hat{f}(x) + \lambda \hat{f}^b(x)$$
    - [(c)] Update the residuals:
$$r_i \leftarrow r_i - \lambda \hat{f}^b(x_i)$$

* [3.] Output the boosted model:
$$\hat{f}(x) = \bar{y}+ \sum_{b=1}^{B} \lambda  \hat{f}^b(x)$$

---

Set `set.seed(123)`. Generate $n=100$ observations from the following model:

* $x_i \sim U(0,2\pi), \quad i=1,\ldots,n$
* $y_i|x_i = \sin(x_i) + \varepsilon_i$
* $\varepsilon_i  \stackrel{i.i.d.}{\sim}N(0,0.25)$

```{r, echo=F}
rm(list=ls())
set.seed(123)
n = 100
x=sort(runif(n)*2*pi)
y=sin(x)+rnorm(n, sd=0.5)
plot(y~x)
curve(sin(x), min(x),max(x), col="red", add=T)
```

* Use the L2-boosting algorithm with $B=100$, $\lambda=0.1$ and $d=1$

---

# 1.

```{r, echo=FALSE}
B = 100+1
# 1.
r = y - mean(y)
fx = matrix(NA, nrow=n, ncol=B)
fx[,1] = rep(mean(y),n)

op <- par(mfrow = c(1, 2))
plot(x,r)
plot(x,y)
curve(sin(x), min(x),max(x), col="red", add=T)
lines(x,fx[,1], type="s", col="blue")
par(op)
```



---

# 2. b=1

```{r, echo=FALSE}
library(rpart)
d = 1
lambda = 0.1
b = 1+1

  # a
  fxb = rpart(r~x, control=rpart.control(maxdepth = d))
  # b
  fx[,b] = fx[,b-1] + lambda*predict(fxb)

# 3.
f.boost = fx[,b]
op <- par(mfrow = c(1, 2))
plot(x,r)
fxb = rpart(r~x, control=rpart.control(maxdepth = d))
lines(x,predict(fxb))
plot(x,y)
curve(sin(x), min(x),max(x), col="red", add=T)
lines(x,f.boost, type="s", col="blue")
par(op)
```

---

# 2. b=2

```{r, echo=F}
set.seed(123)
x=sort(runif(n)*2*pi)
y=sin(x)+rnorm(n, sd=0.5)
B = 1+2
# 1.
r = y - mean(y)
fx = matrix(NA, nrow=n, ncol=B)
fx[,1] = rep(mean(y),n)
# 2.
for (b in 2:B){
  # a
  fxb = rpart(r~x, control=rpart.control(maxdepth = d))
  # b
  fx[,b] = fx[,b-1] + lambda*predict(fxb)
  # c
  r = r - lambda*predict(fxb)
}
# 3.
f.boost = fx[,B]
op <- par(mfrow = c(1, 2))
plot(x,r)
fxb = rpart(r~x, control=rpart.control(maxdepth = d))
lines(x,predict(fxb))
plot(x,y)
curve(sin(x), min(x),max(x), col="red", add=T)
lines(x,f.boost, type="s", col="blue")
par(op)
```

---

# 2. b=5

```{r, echo=F}
set.seed(123)
x=sort(runif(n)*2*pi)
y=sin(x)+rnorm(n, sd=0.5)
B = 1+5
# 1.
r = y - mean(y)
fx = matrix(NA, nrow=n, ncol=B)
fx[,1] = rep(mean(y),n)
# 2.
for (b in 2:B){
  # a
  fxb = rpart(r~x, control=rpart.control(maxdepth = d))
  # b
  fx[,b] = fx[,b-1] + lambda*predict(fxb)
  # c
  r = r - lambda*predict(fxb)
}
# 3.
f.boost = fx[,B]
op <- par(mfrow = c(1, 2))
plot(x,r)
fxb = rpart(r~x, control=rpart.control(maxdepth = d))
lines(x,predict(fxb))
plot(x,y)
curve(sin(x), min(x),max(x), col="red", add=T)
lines(x,f.boost, type="s", col="blue")
par(op)
```

---

# 2. b=10

```{r, echo=F}
set.seed(123)
x=sort(runif(n)*2*pi)
y=sin(x)+rnorm(n, sd=0.5)
B = 1+10
# 1.
r = y - mean(y)
fx = matrix(NA, nrow=n, ncol=B)
fx[,1] = rep(mean(y),n)
# 2.
for (b in 2:B){
  # a
  fxb = rpart(r~x, control=rpart.control(maxdepth = d))
  # b
  fx[,b] = fx[,b-1] + lambda*predict(fxb)
  # c
  r = r - lambda*predict(fxb)
}
# 3.
f.boost = fx[,B]
op <- par(mfrow = c(1, 2))
plot(x,r)
fxb = rpart(r~x, control=rpart.control(maxdepth = d))
lines(x,predict(fxb))
plot(x,y)
curve(sin(x), min(x),max(x), col="red", add=T)
lines(x,f.boost, type="s", col="blue")
par(op)
```

---

# 2. b=B

```{r, echo=F}
set.seed(123)
x=sort(runif(n)*2*pi)
y=sin(x)+rnorm(n, sd=0.5)
B = 1+100
# 1.
r = y - mean(y)
fx = matrix(NA, nrow=n, ncol=B)
fx[,1] = rep(mean(y),n)
# 2.
for (b in 2:B){
  # a
  fxb = rpart(r~x, control=rpart.control(maxdepth = d))
  # b
  fx[,b] = fx[,b-1] + lambda*predict(fxb)
  # c
  r = r - lambda*predict(fxb)
}
# 3.
f.boost = fx[,B]
op <- par(mfrow = c(1, 2))
plot(x,r)
fxb = rpart(r~x, control=rpart.control(maxdepth = d))
lines(x,predict(fxb))
plot(x,y)
curve(sin(x), min(x),max(x), col="red", add=T)
lines(x,f.boost, type="s", col="blue")
par(op)
```

---

# Tuning parameters for boosting


* The __number of trees__ $B$: Unlike bagging and random forests, boosting can overfit if $B$ is too large, although this overfitting tends to occur slowly if at all. We will use cross-validation to select $B$

* The __shrinkage parameter__ $\lambda$: A small positive number, which controls the rate at which boosting learns. Typical values
are 0.01 or 0.001, and the right choice can depend on the
problem. Very small $\lambda$ can require using a very large value
of $B$ in order to achieve good performance

* The __number of splits__ $d$:
 It controls the complexity of the boosted ensemble. Often $d = 1$ works well, in which case each tree consists of a
single split (a __stump__), resulting in an __additive model__. More
generally $d$ is the interaction depth, and controls the
interaction order of the boosted model, since $d$ splits can
involve at most $d$ variables

---

# gbm

```{r, eval=FALSE}
library(gbm)
fit <- gbm(y ~ ., 
           distribution = "gaussian",
           data = train,
           n.trees = B,
           interaction.depth = d,
           shrinkage = lambda,
           bag.fraction = 0.5, # default
           cv.folds = 0)  # default
```

* `bag.fraction = 0.5` : grows each new tree on a 50% random sub-sample of the training data. Apart from speeding up the computations, this has a similar effect to bagging, and results in some variance reduction in the ensemble

* `cv.folds = 0` : no cross-validation

---

# Number of trees 

```{r, echo=FALSE, fig.align = 'center', out.width = '50%', out.height = '50%'}
knitr::include_graphics("images/Fig17_6.png")
```

.center[Efron and Hastie (2016), Figure 17.6: we see that as the number of trees $B$ gets large, the test
error for boosting starts to increase (overfitting)]

---

# Shrinkage parameter 

```{r, echo=FALSE, fig.align = 'center', out.width = '60%', out.height = '60%'}
knitr::include_graphics("images/Fig17_10.png")
```

.center[Efron and Hastie (2016), Figure 17.10: Boostedmodels with different shrinkage parameters. The solid curves are
validation errors, the dashed curves training errors]

---

# Number of splits 

```{r, echo=FALSE, fig.align = 'center', out.width = '60%', out.height = '60%'}
knitr::include_graphics("images/Fig17_8.png")
```

.center[Efron and Hastie (2016), Figure 17.8: it appears that $d=1$ is inferior to the rest, with $d=4$
about the best. With $d=7$, overfitting begins around 200 trees,
with $d=4$ around 300, while neither of the other two show
evidence of overfitting by 500 trees]

---

# Boosted stumps

* Suppose we have a fitted boosted model $\hat{f}(x)$ using $B$ regression trees with $d=1$ 

* Denote by $\mathcal{B}_j \subseteq \{1,\ldots,B\}$ the indices of the trees that made the single split using the predictor $x_j$, for $j=1,\ldots,p$

* These $\mathcal{B}_j$ are disjoint (some $\mathcal{B}_k$ can be empty) and $\bigcup_{j=1}^{p}\mathcal{B}_j = \{1,\ldots,B\}$

*  Then we can write
$$
\begin{aligned}
\hat{f}(x) &= \bar{y} + \sum_{b=1}^{B} \lambda \hat{f}^{b}(x)\\
&= \bar{y} + \sum_{j=1}^{p} \sum_{b\in \mathcal{B}_j} \lambda \hat{f}^{b}(x_j) \\
&= \bar{y} + \sum_{j=1}^{p}  \hat{f}_j(x_j)
\end{aligned}
$$

---

# Boosted stumps 

```{r, echo=FALSE, fig.align = 'center', out.width = '60%', out.height = '60%'}
knitr::include_graphics("images/Fig17_9.png")
```

.center[Efron and Hastie (2016), Figure 17.9: Three of the fitted functions, in a boosted stumps model]


---

# Variable importance 

```{r, echo=FALSE, fig.align = 'center', out.width = '40%', out.height = '40%'}
knitr::include_graphics("images/Fig17_7.png")
```

.center[Efron and Hastie (2016), Figure 17.7: here 267 of the 369 variables were used]




---

# ALS data

* From CASI, Chapter 17: Random Forests and Boosting

* ALS data represent measurements on patients with amyotrophic lateral
sclerosis (Lou Gehrigâ€™s disease)

* The goal is to predict the rate of progression
of an ALS functional rating score (FRS)

* There are $n=1197$ training measurements on $p=369$ predictors and the response, with a corresponding test set of size $m=625$ observations


---


```{r}
rm(list=ls())
# data import
als <- read.table("http://web.stanford.edu/~hastie/CASI_files/DATA/ALS.txt",header=TRUE)
# split into training and test
train = als[als$testset==F,-1]
n = nrow(train)
test = als[als$testset==T,-1]
m = nrow(test)
```

Use the function `gbm()` to fit a boosting model for the ALS data by setting 
* $B = 500$
* $\lambda = 0.02$
* $d = 4$ 

---

```{r, echo=FALSE}
# library gbm
library(gbm)
set.seed(123)

# tuning parameters
d = 4
nu = 0.02
B = 500
K = 10

# boosted regression trees fit
fit <- gbm(dFRS ~ ., 
             distribution = "gaussian",
             data = train,
             n.trees = B,
             interaction.depth = d,
             shrinkage = nu,
             bag.fraction = 1, 
             cv.folds= K)

print(fit)
```


---

```{r, echo=F}
# predictions at each step b
yhats = sapply(1:B, function(b) predict(fit, newdata = test, n.trees = b))

# test MSE at each step b
MSEs = apply(yhats, 2, function(yhat) mean( (test$dFRS - yhat)^2))
plot(1:B, MSEs, xlab="Number of trees", ylab="test MSE", type="l")
which.min(MSEs)
```

---

```{r, echo=F}
# bstop
bstop = gbm.perf(fit, method="cv")
```

---

```{r, echo=F}
# variable importante
summary(fit, n.trees=bstop)[1:10,]
```

---

```{r, echo=FALSE}
# fitted functions
plot(fit, i.var="Onset.Delta", n.trees=bstop)
```

---

```{r, echo=FALSE}
plot(fit, i.var="last.slope.weight", n.trees=bstop)
```
