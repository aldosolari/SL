<!DOCTYPE html>
<html>
  <head>
    <title>Sample-Splitting Inference</title>
    <meta charset="utf-8">
    <meta name="author" content="Aldo Solari" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <strong>Sample-Splitting Inference</strong>
### Aldo Solari

---





# Outline

* High-dimensional inference

* Inference after model selection

* Single-split inference

* Multi-split inference

---

# Linear model

Consider the linear model
$$
y = \mathcal{N}(1_n \beta_0 + X\beta, \sigma^2I_n)
$$
where

* `\(\underset{n\times 1}{y} = (y_1,\ldots,y_n)'\)` is the response on `\(n\)` observations

* `\(\underset{n\times p}{X}\)` is the (fixed or random) design matrix containing the measurements on `\(p\)` variables 

* `\(\underset{p\times 1}{\beta} = (\beta_1,\ldots,\beta_p)'\)` is the vector of coefficients of interest

* `\(\beta_0\)` and `\(\sigma^2\)` are nuisance parameters

* `\(\underset{n\times 1}{1_n} = (1,1,\ldots,1)'\)` is a vector of ones of length `\(n\)` and `\(\underset{n\times n}{I_n}\)` is the identity matrix

* Here the main assumption is that the linear model is correct. This might be rather unrealistic

---

# High-dimensional inference

* Goal: obtain confidence intervals `\(\mathrm{IC}_j\)` and `\(p\)`-values `\(p_j\)` for testing the null hypothesis `\(H_j: \beta_j = 0\)` for the individual regression parameters `$$\beta_1,\ldots,\beta_p$$`

* The *low-dimensional* setting
`$$p &lt; n$$`
makes statistical inference easier because you can fit the full model.
Methods like Bonferroni and Benjamini-Yekutieli can be used to take into account the *multiplicity* of confidence intervals/p-values

* The *high-dimensional* setting `$$p \geq n$$` 
makes statistical inference very challenging. This problem is known as **high-dimensional inference**

---

# Inference after model selection

* One classical problem is **variable selection**, i.e. obtain 
`$$\hat{S} \subset \{1,\ldots,p\}$$`
such that the number of wrong selections `\(\hat{S} \setminus S\)` is low and/or the number of correct selections `\(\hat{S} \cap S\)` is high

* A follow-up problem is to perform **inference after model selection**, i.e. obtain confidence intervals and `\(p\)`-values for the selected variables `$$\beta_j \in \hat{S}$$`
Statistical inference after model selection requires an inference framework that takes the *selection* into account in order to be valid

* High-dimensional inference can be performed in two steps:
    1. variable selection
    2. inference after model selection

---

# Simulation set-up 

* `\(n=200\)`

* `\(p=2000\)`

* `\(\beta_0 = 1\)`

* `\(\sigma^2 = 1\)`

* `\(s = 10\)`

* `\(\beta_{j}=U(1,3)\)` for `\(j=1,\ldots,s\)`, `\(\beta_{j}=0\)` for `\(j=s+1,\ldots,p\)`

* `\(X \sim \mathcal{N}(0,\Sigma)\)`

* `\(\Sigma\)` with 1 diagonal elements and `\(\rho\)` off-diagonal elements

---


```r
set.seed(123)
n = 200
p = 2000
s = 10
beta = c(runif(s,1,3),rep(0,p-s))
rho = 0
Sigma = matrix(rho,ncol=p,nrow=p) + diag(rep(1-rho,p))
X &lt;- as.matrix(matrix(rnorm(n * p), n, p) %*% Sigma)
y &lt;- X %*% beta + rnorm(n, mean = 1)
varType &lt;- rep("N",p)
varType[1:s] &lt;- "I"
colnames(X) &lt;- paste0("x", 1:p)
yX = data.frame(y,X)
```

---

# Naive approach


```r
# Variable selection by lasso
library(glmnet)
set.seed(123)
vs &lt;- cv.glmnet(X, y)
hatS &lt;- which(coef(vs, s=vs$lambda.min)[-1] != 0)
table(varType[hatS])
```

```

 I  N 
10 90 
```

```r
# Inference after model selection using the same observations
fitS &lt;- lm(y ~ X[,hatS])
pvalS &lt;- summary(fitS)$coef[-1,4]
alpha = 0.05
table(varType[hatS][pvalS &lt;= alpha])
```

```

 I  N 
10 40 
```

---

# Bonferroni after model selection

Bonferroni after model selection corrects for the multiplicity of the `\(\#\hat{S}\)` selected test
`$$p_j \leq \frac{\alpha}{\#\hat{S}}, \qquad j \in \hat{S}$$`
but fails to control the FWER


```r
bonf = p.adjust(pvalS,"bonf") &lt;= alpha
table(varType[hatS][bonf])
```

```

 I  N 
10  7 
```

---


# Single-split inference

* A generic way for
deriving confidence intervals/$p$-values is given by *sample-splitting inference*

* Single-split inference requires
splitting the observations with indices `\(\{1,\ldots,n\}\)` in two equal halves denoted by `\(I_1\)` and `\(I_2\)`, that is, `\(I_r \subset \{1,\ldots,n\}\)`, `\(r=1,2\)` with `\(I_1 \cup I_2 = \{1,\ldots,n\}\)` and
`\(I_1 \cap I_2=\emptyset\)`

* The idea
is
    1. Use the first half of observations `\(I_1\)` for variable selection 
    2. Use the second half of observations `\(I_2\)` with the reduced set of selected variables `\(\hat{S}\)` for statistical inference 

* Such a sample-splitting procedure avoids the
*over-optimism* to use the data twice for selection and
inference after selection (without taking the effect of
selection into account)

---


* Consider a method for variable selection based on
the first half of the sample:

`$$\hat{S}(I_1) \subset \{1,\ldots,p\}$$`


```r
set.seed(123)
I1 &lt;- as.logical(sample(rep(0:1, each=n/2)))
```

* A prime example is the Lasso which selects all the variables
whose corresponding estimated regression coefficients
are different from zero:


```r
set.seed(123)
vs &lt;- cv.glmnet(X[I1,], y[I1])
hatS &lt;- which(coef(vs, s=vs$lambda.min)[-1]!=0)
table(varType[hatS])
```

```

 I  N 
10 58 
```

---

* We then use the second
half of the sample `\(I_2\)` for constructing confidence intervals/p-values,
based on the selected variables `\(\hat{S}(I_1)\)`.

* If the cardinality `\(\# \hat{S}(I_1) \leq n/2\)`, we can run ordinary least squares estimation using the subsample `\(I_2\)` and the
selected variables `\(\hat{S}(I_1)\)`, that is, we regress `\(y_{I_2}\)` on `\(X_{I_2}^{\hat{S}(I_1)}\)`
where the sub-indices denote the sample half
and the super-index stands for the selected variables,
respectively


```r
XS &lt;- X[!I1, hatS]
fit &lt;- lm(y[!I1]~XS)
```


* Thus, from such a procedure, we obtain `\(p\)`-values `\(p_j\)` for testing
`\(H_j: \beta_j = 0\)` for `\(j \in \hat{S}(I_1)\)`

* We define `\(p\)`-values `\(p_j=1\)` for `\(j \notin \hat{S}(I_1)\)` 


```r
pval = rep(1,p)
pval[hatS] = summary(fit)$coefficients[-1,4]
table(varType[pval &lt;= alpha])
```

```

 I  N 
10  2 
```

---

# Multiplicity adjustment

* If we wish to control the FWER over all considered hypotheses `\(H_j\)`, `\(j =1,\ldots,p\)`, a
naive approach would employ a Bonferroni correction
over the `\(p\)` tests

* This is not necessary: we only need to control over the considered `\(\# \hat{S}(I_1)\)` tests in `\(I_2\)`

* Such corrected `\(p\)`-values control
the familywise error rate in multiple testing when assuming
the **screening property**:
`$$\hat{S} \supseteq S$$`
for the selector `\(\hat{S} = \hat{S}(I_1)\)` based on the first half `\(I_1\)` only



```r
holm = p.adjust(pval[hatS],"holm") &lt;= alpha
table(varType[hatS][holm])
```

```

 I 
10 
```

---

# P-value lottery

* A major problem of the single sample-splitting
method is its sensitivity with respect to the choice
of splitting the entire sample

* Different sample splits lead to
wildly different `\(p\)`-values

* This undesirable phenomenon is called
a `\(p\)`-value *lottery*

---


```r
set.seed(123)
B = 50
P &lt;- matrix(1, B, p)
for (i in 1:B) {
  split &lt;- as.logical(sample(rep(0:1, each=n/2)))
  vs &lt;- cv.glmnet(X[split,], y[split])
  hatS &lt;- which( coef(vs, s=vs$lambda.min)[-1] !=0 )
  XS &lt;- X[!split, hatS]
  fit &lt;- lm(y[!split]~XS)
  P[i, hatS] &lt;- summary(fit)$coeff[-1,4]
}
```

---


```r
hist(P[,1])
```

&lt;img src="10_Split_files/figure-html/unnamed-chunk-10-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---

# Multiple-split inference

* To overcome the `\(p\)`-value lottery, we can run the sample-splitting method `\(B\)` times, with `\(B\)` large

* Thus, we obtain a collection of `\(p\)`-values for the `\(j\)`th hypothesis `\(H_j\)`
`$$p_j^{[1]},\ldots,p_j^{[B]}$$`

* The task is now to do an aggregation to a single `\(p\)`-value. Because of dependence among `\(\{p_j^{[B]},b=1,\ldots,B\}\)`, because all the different half samples are
part of the same full sample, an appropriate aggregation is needed

* A simple solution is to use the median of `\(\{p_j^{[B]},b=1,\ldots,B\}\)` and multiplying it with the factor 2

---

# R package hdi

The implementation in the R package hdi works as follow:


```r
library(hdi)
set.seed(123)
fit &lt;- multi.split(x=X, y=y, B=50, fraction=0.5, ci=TRUE, ci.level = 0.95)
fit
```

```
alpha = 0.01: Selected predictors: 1 2 3 4 5 6 7 8 9 10 
alpha = 0.05: Selected predictors: 1 2 3 4 5 6 7 8 9 10 
------
Familywise error rate controlled at level alpha.
```

---

* To obtain adjusted `\(p\)`-values for `\(H_j\)` controlling the familywise error rate:


```r
p.fwer = fit$pval.corr
```

* Confidence intervals can be constructed based on the
duality with the `\(p\)`-values:


```r
cbind(beta[1:s],confint(fit, parm=which(p.fwer &lt;= 0.05), level=0.95))
```

```
                 lower    upper
x1  1.575155 1.3695223 1.912305
x2  2.576610 2.3392706 2.940036
x3  1.817954 1.4798431 2.075181
x4  2.766035 2.5221155 3.033342
x5  2.880935 2.4921017 2.978815
x6  1.091113 0.7677466 1.380896
x7  2.056211 1.8965279 2.416621
x8  2.784838 2.5459804 3.039660
x9  2.102870 1.8199164 2.349929
x10 1.913229 1.7737531 2.272846
```
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLanguage": "R",
"countIncrementalSlides": false,
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
