---
title: "**Sample-Splitting Inference**"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true    
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, 
                      comment=NA, cache=F, R.options=list(width=220),
                      fig.align='center', out.width='60%', fig.asp=.7)
```


# Outline

* High-dimensional inference

* Inference after model selection

* Single-split inference

* Multi-split inference

---

# Linear model

Consider the linear model
$$
y = \mathcal{N}(1_n \beta_0 + X\beta, \sigma^2I_n)
$$
where

* $\underset{n\times 1}{y} = (y_1,\ldots,y_n)'$ is the response on $n$ observations

* $\underset{n\times p}{X}$ is the (fixed or random) design matrix containing the measurements on $p$ variables 

* $\underset{p\times 1}{\beta} = (\beta_1,\ldots,\beta_p)'$ is the vector of coefficients of interest

* $\beta_0$ and $\sigma^2$ are nuisance parameters

* $\underset{n\times 1}{1_n} = (1,1,\ldots,1)'$ is a vector of ones of length $n$ and $\underset{n\times n}{I_n}$ is the identity matrix

* Here the main assumption is that the linear model is correct. This might be rather unrealistic

---

# High-dimensional inference

* Goal: obtain confidence intervals $\mathrm{IC}_j$ and $p$-values $p_j$ for testing the null hypothesis $H_j: \beta_j = 0$ for the individual regression parameters $$\beta_1,\ldots,\beta_p$$

* The *low-dimensional* setting
$$p < n$$
makes statistical inference easier because you can fit the full model.
Methods like Bonferroni and Benjamini-Yekutieli can be used to take into account the *multiplicity* of confidence intervals/p-values

* The *high-dimensional* setting $$p \geq n$$ 
makes statistical inference very challenging. This problem is known as **high-dimensional inference**

---

# Inference after model selection

* One classical problem is **variable selection**, i.e. obtain 
$$\hat{S} \subset \{1,\ldots,p\}$$
such that the number of wrong selections $\hat{S} \setminus S$ is low and/or the number of correct selections $\hat{S} \cap S$ is high

* A follow-up problem is to perform **inference after model selection**, i.e. obtain confidence intervals and $p$-values for the selected variables $$\beta_j \in \hat{S}$$
Statistical inference after model selection requires an inference framework that takes the *selection* into account in order to be valid

* High-dimensional inference can be performed in two steps:
    1. variable selection
    2. inference after model selection

---

# Simulation set-up 

* $n=200$

* $p=2000$

* $\beta_0 = 1$

* $\sigma^2 = 1$

* $s = 10$

* $\beta_{j}=U(1,3)$ for $j=1,\ldots,s$, $\beta_{j}=0$ for $j=s+1,\ldots,p$

* $X \sim \mathcal{N}(0,\Sigma)$

* $\Sigma$ with 1 diagonal elements and $\rho$ off-diagonal elements

---

```{r}
set.seed(123)
n = 200
p = 2000
s = 10
beta = c(runif(s,1,3),rep(0,p-s))
rho = 0
Sigma = matrix(rho,ncol=p,nrow=p) + diag(rep(1-rho,p))
X <- as.matrix(matrix(rnorm(n * p), n, p) %*% Sigma)
y <- X %*% beta + rnorm(n, mean = 1)
varType <- rep("N",p)
varType[1:s] <- "I"
colnames(X) <- paste0("x", 1:p)
yX = data.frame(y,X)
```

---

# Naive approach

```{r}
# Variable selection by lasso
library(glmnet)
set.seed(123)
vs <- cv.glmnet(X, y)
hatS <- which(coef(vs, s=vs$lambda.min)[-1] != 0)
table(varType[hatS])
# Inference after model selection using the same observations
fitS <- lm(y ~ X[,hatS])
pvalS <- summary(fitS)$coef[-1,4]
alpha = 0.05
table(varType[hatS][pvalS <= alpha])
```

---

# Bonferroni after model selection

Bonferroni after model selection corrects for the multiplicity of the $\#\hat{S}$ selected test
$$p_j \leq \frac{\alpha}{\#\hat{S}}, \qquad j \in \hat{S}$$
but fails to control the FWER

```{r}
bonf = p.adjust(pvalS,"bonf") <= alpha
table(varType[hatS][bonf])
```

---


# Single-split inference

* A generic way for
deriving confidence intervals/$p$-values is given by *sample-splitting inference*

* Single-split inference requires
splitting the observations with indices $\{1,\ldots,n\}$ in two equal halves denoted by $I_1$ and $I_2$, that is, $I_r \subset \{1,\ldots,n\}$, $r=1,2$ with $I_1 \cup I_2 = \{1,\ldots,n\}$ and
$I_1 \cap I_2=\emptyset$

* The idea
is
    1. Use the first half of observations $I_1$ for variable selection 
    2. Use the second half of observations $I_2$ with the reduced set of selected variables $\hat{S}$ for statistical inference 

* Such a sample-splitting procedure avoids the
*over-optimism* to use the data twice for selection and
inference after selection (without taking the effect of
selection into account)

---


* Consider a method for variable selection based on
the first half of the sample:

$$\hat{S}(I_1) \subset \{1,\ldots,p\}$$

```{r}
set.seed(123)
I1 <- as.logical(sample(rep(0:1, each=n/2)))
```

* A prime example is the Lasso which selects all the variables
whose corresponding estimated regression coefficients
are different from zero:

```{r}
set.seed(123)
vs <- cv.glmnet(X[I1,], y[I1])
hatS <- which(coef(vs, s=vs$lambda.min)[-1]!=0)
table(varType[hatS])
```

---

* We then use the second
half of the sample $I_2$ for constructing confidence intervals/p-values,
based on the selected variables $\hat{S}(I_1)$.

* If the cardinality $\# \hat{S}(I_1) \leq n/2$, we can run ordinary least squares estimation using the subsample $I_2$ and the
selected variables $\hat{S}(I_1)$, that is, we regress $y_{I_2}$ on $X_{I_2}^{\hat{S}(I_1)}$
where the sub-indices denote the sample half
and the super-index stands for the selected variables,
respectively

```{r}
XS <- X[!I1, hatS]
fit <- lm(y[!I1]~XS)
```


* Thus, from such a procedure, we obtain $p$-values $p_j$ for testing
$H_j: \beta_j = 0$ for $j \in \hat{S}(I_1)$

* We define $p$-values $p_j=1$ for $j \notin \hat{S}(I_1)$ 

```{r}
pval = rep(1,p)
pval[hatS] = summary(fit)$coefficients[-1,4]
table(varType[pval <= alpha])
```

---

# Multiplicity adjustment

* If we wish to control the FWER over all considered hypotheses $H_j$, $j =1,\ldots,p$, a
naive approach would employ a Bonferroni correction
over the $p$ tests

* This is not necessary: we only need to control over the considered $\# \hat{S}(I_1)$ tests in $I_2$

* Such corrected $p$-values control
the familywise error rate in multiple testing when assuming
the **screening property**:
$$\hat{S} \supseteq S$$
for the selector $\hat{S} = \hat{S}(I_1)$ based on the first half $I_1$ only


```{r}
holm = p.adjust(pval[hatS],"holm") <= alpha
table(varType[hatS][holm])
```

---

# P-value lottery

* A major problem of the single sample-splitting
method is its sensitivity with respect to the choice
of splitting the entire sample

* Different sample splits lead to
wildly different $p$-values

* This undesirable phenomenon is called
a $p$-value *lottery*

---

```{r}
set.seed(123)
B = 50
P <- matrix(1, B, p)
for (i in 1:B) {
  split <- as.logical(sample(rep(0:1, each=n/2)))
  vs <- cv.glmnet(X[split,], y[split])
  hatS <- which( coef(vs, s=vs$lambda.min)[-1] !=0 )
  XS <- X[!split, hatS]
  fit <- lm(y[!split]~XS)
  P[i, hatS] <- summary(fit)$coeff[-1,4]
}
```

---

```{r}
hist(P[,1])
```

---

# Multiple-split inference

* To overcome the $p$-value lottery, we can run the sample-splitting method $B$ times, with $B$ large

* Thus, we obtain a collection of $p$-values for the $j$th hypothesis $H_j$
$$p_j^{[1]},\ldots,p_j^{[B]}$$

* The task is now to do an aggregation to a single $p$-value. Because of dependence among $\{p_j^{[B]},b=1,\ldots,B\}$, because all the different half samples are
part of the same full sample, an appropriate aggregation is needed

* A simple solution is to use the median of $\{p_j^{[B]},b=1,\ldots,B\}$ and multiplying it with the factor 2

---

# R package hdi

The implementation in the R package hdi works as follow:

```{r}
library(hdi)
set.seed(123)
fit <- multi.split(x=X, y=y, B=50, fraction=0.5, ci=TRUE, ci.level = 0.95)
fit
```

---

* To obtain adjusted $p$-values for $H_j$ controlling the familywise error rate:

```{r}
p.fwer = fit$pval.corr
```

* Confidence intervals can be constructed based on the
duality with the $p$-values:

```{r}
cbind(beta[1:s],confint(fit, parm=which(p.fwer <= 0.05), level=0.95))
```


