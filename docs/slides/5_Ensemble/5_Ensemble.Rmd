---
title: "**Ensemble learning**"
author: Aldo Solari
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
      highlightLines: true    
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval=T, message=F, warning=F, error=F, 
                      comment=NA, cache=F, R.options=list(width=220),
                      fig.align='center', out.width='60%', fig.asp=.7)
```


# Outline

* AdaBoost

* Gradient boosting 

* Model stacking

---

# Adaboost

* Boosting was originally proposed as a means for improving the performance
of weak learners in binary classification problems

* The idea is to fit a sequence of classifiers to modified versions of the training data, where the modifications give more weight to misclassified observations

* The final classification is by weighted majority vote

* We present the AdaBoost.M1 algorithm (Freund
and Schapire, 1997), which was developed for the response variable coded as $Y \in \{-1,1\}$ and uses decision trees as weak learners

---

# Adaboost algorithm for classification trees

1. Initialize the observation weights $w_i=1/n, i=1,\ldots,n$

2. For $b=1,\ldots,B$ repeat the following steps

    - Fit a classification tree $\hat{C}^{b}$ to the training data, using observation weights $w_i$
    - Compute the weighted misclassification error for $\hat{C}^{b}$
$$\mathrm{Err}^{b} = \frac{\sum_{i=1}^{n}w_i  I\{ y_i \neq \hat{C}^{b}(x_i) \}}{\sum_{i=1}^{n}w_i}$$
    - Compute 
    $$\alpha^{b} = \log[ (1- \mathrm{Err}^{b})/\mathrm{Err}^{b}]$$

    -  Update the weights as
$$w_i \leftarrow w_i \cdot  \exp(\alpha^{b} \cdot I\{y_i \neq \hat{C}^{b}(x_i) \} ), \quad i=1,\ldots,n$$


3. Output  $\hat{C}^{B}(x) = \mathrm{sign}( \sum_{b=1}^B \alpha^{b} \hat{C}^{b}(x) )$

---

# Step 2, b=1

![](images/toy1.pdf)

![](images/toy2.pdf)

---

# Step 2, b=2 and 3

![](images/toy3.pdf)

![](images/toy4.pdf)

---

# Step 3

![](images/toy5.pdf)

---

# Gradient boosting algorithm (gbm)

1. Start with $\hat{f}^0(x)=0$, and set $B$ and the shrinkage parameter $\lambda>0$

2. For $b=1,\ldots,B$ repeat the following steps:

    (a) Compute the pointwise negative gradient of the __loss function__ at the current fit
$$r_i = -\frac{\partial L(y_i,f_i)}{\partial f_i}\left|_{f_i=\hat{f}^{b-1}(x_i)}\right., \quad i=1,\ldots,n$$

    (b) Approximate the negative gradient by a tree $g$ with depht $d$:
    $$(x_1,r_1),\ldots,(x_n,r_n)\rightarrow \hat{g}(x)$$
    
    (c) Update 
    $$\hat{f}^b(x) = \hat{f}^{b-1}(x) + \lambda \hat{g}(x)$$
    
3. Return the sequence $\hat{f}^b(x), b=1,\ldots,B$

---

# Loss functions

* The R package `gbm` implements the previous Algorithm for a variety
of loss functions $L$, including 
    - __Squared-error__: $L(y,f(x)) = \frac{1}{2} (y - f(x))^2$ with negative gradient $$- \frac{\partial}{\partial f(x)} \frac{1}{2}[y - f(x)]^2 = y - f(x)$$
    - __Laplace__ $L(y,f(x)) =  |y - f(x)|$ with negative gradient 
    $$- \frac{\partial}{\partial f(x)} \,\, | y - f(x)| = \mathrm{sign}(y - f(x))$$
Note that the Laplace loss is not differentiable at $y=f(x)$; the value of the negative gradient at such points is set to 0

* It turns out that the Adaboost Algorithm for the classification
problem with $\tilde{y}\in \{-1,1\}$ fits a logistic regression
model using an __exponential
loss__ $L(\tilde{y},f(x)) =  \exp(-\tilde{y} f(x))$ with negative gradient
$$- \frac{\partial}{\partial f(x)} \,\,  \exp[-\tilde{y} f(x) ] = \tilde{y} \exp[-\tilde{y} f(x) ]$$

---

# Boosting tuning parameters

* Boosting may outperforms a random forest, but at a price

* Careful tuning of boosting requires considerable extra work,
with time-costly rounds of cross-validation, whereas random forests are almost
automatic

---

# Boston data

```{r, eval=FALSE}
# load required libraries
rm(list=ls())
library(MASS)
library(caret)

# import data
set.seed(123)
split <- createDataPartition(y=Boston$medv, p = 0.7, list=FALSE)
train <- Boston[split,]
test = Boston[-split,-14]
test.y = Boston[-split,14]

# cross-validation
cv <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 2
)
```

---

```{r, eval=FALSE}
rf <- train(
  medv~., train,
  method = "rf",
  trControl=cv)

# tuning of boosting
grid <- expand.grid(
  n.trees = c(500, 1000, 1500),
  shrinkage = c(0.01, 0.05),
  interaction.depth = c(2,4,5,6), 
  n.minobsinnode = c(5,10)
)
boost <- train(
  medv~., train,
  method = "gbm",
  tuneGrid = grid,
  verbose = FALSE,
  trControl=cv)

plot(boost)
boost$bestTune
```

---
  
```{r, eval=FALSE}
models = list(
    rf=rf,
    boost=boost
  )

resamps = resamples(models)
bwplot(resamps, metric = "RMSE")

yhats = predict(models, newdata=test)
lapply(yhats, function(yhat) sqrt( mean( (yhat - test.y)^2) ) )

predVals <- extractPrediction(models, 
                              testX = test, 
                              testY = test.y)

plotObsVsPred(predVals)
```

---

# Ensemble learning

* Ensemble methods are techniques that create multiple models and then combine them to produce improved results

* These models, when used as inputs of ensemble methods, are often called __weak learners__

*  Ensemble learning is appealing because that it is able to boost weak learners which are slightly better than random guess to strong learners which can make very accurate predictions

* We have already seen some examples of ensemble learning: __bagging__, __random forests__ and __boosting__ grow multiple trees (weak learners) which are then combined to yield a single prediction

* However, ensemble learning increases computation time and reduces interpretability

---

# Model stacking

* Stacking is a general method to combine models 

* Consider a __library__ of $L$ models $\hat{f}_1,\ldots, \hat{f}_L$ fitted on the training data
$$(x_1,y_1),\ldots, (x_n,y_n)$$


* Perhaps by combining their respective efforts, we could get even better prediction than using any
particular one

* The issue then how we might combine them for predicting the test set $y^*_1,\ldots,y^*_m$

* A linear combination 
$$\hat{y}^{*}_i = \sum_{l=1}^{L}w_l \hat{f}_l(x^*_i)$$
requires to define the the weights $w_1,\ldots,w_L$

---

# Least squares

* The method of least squares provides the weights 
$$\hat{w}_1,\ldots,\hat{w}_L=  \underset{w_1,\ldots,w_L}{\arg \min\,\,} \sum_{i=1}^{n} \left[ y_i - \sum_{l=1}^{L} w_l \hat{f}_l(x_i) \right]^2$$

* However, in this way we fail to take into account for model complexity: models with higher complexity get higher weights

* For example, consider $L$ predictors and let $\hat{f}_{l}$ be the linear model formed the best subset of predictors of size $l$, $l=1,\ldots,L$, where best is defined as having the smallest $\mathrm{MSE}_{\mathrm{Tr}}$

* Then all the weight goes on the largest model, that is, $\hat{w}_L = 1$ and $\hat{w}_l = 0$ for $l< L$

---

# Stacked regression

* Wolpert (1992) presented an interesting idea, called __stacked generalizations__.  This proposal was translated in statistical language by Breiman, in 1993

* If we exclude $y_i$ in the fitting procedure of the models, then 
 $\hat{f}^{-i}_1(x_i),\ldots, \hat{f}^{-i}_L(x_i)$ do not depend on $y_i$ 

* __Stacked regression__ is a particular case of the model stacking algorithm (next) with $\hat{f}_{\mathrm{stack}}=$ linear model and cross-validation with $K=n$

* Read [Guide to Model Stacking](https://gormanalysis.com/guide-to-model-stacking-i-e-meta-ensembling/
) by Ben Gorman

---

# Staked regression algorithm

1. Let $\hat{f}^{-i}_l(x_i)$ be the prediction at $x_i$ using model $l$ fitted to the training data with the
$i$th training observation $(x_i,y_i)$ removed

2. Obtain the weights by least squares
$$\hat{w}_1,\ldots,\hat{w}_L = \underset{w_1,\ldots,w_L}{\arg \min} \sum_{i=1}^{n} \left[ y_i - \sum_{l=1}^{L} w_l \hat{f}^{-i}_l(x_i) \right]^2$$

3. Compute the predictions for the test data as
$$\hat{f}_{\mathrm{stack}}(x^*_i) = \sum_{l=1}^{L} \hat{w}_l  \hat{f}_l(x^*_i), \quad i=1,\ldots,m$$

---

# Model stacking algorithm

1. Partition the training data into $K$ folds $\mathcal{F}_1,\ldots,\mathcal{F}_K$

2. For each test fold $\mathcal{F}_k$, $k=1,\ldots,K$ ,combine the other $K-1$ folds to be used as a training fold 
    - For $l=1,\ldots,L$,  fit the $l$th model to the training fold and make predictions on the test fold $\mathcal{F}_k$. Store these predictions $$z_i = (\hat{f}_1^{-\mathcal{F}_k}(x_i),\ldots,\hat{f}_L^{-\mathcal{F}_k}(x_i)), \quad  i \in \mathcal{F}_k$$

3. For $l=1,\ldots,L$,  fit the $l$th model to the full training data and make predictions on the test data. Store these predictions
$$z^*_i = (\hat{f}_1(x^*_i),\ldots,\hat{f}_L(x^*_i)), \quad i=1,\ldots,m$$


4. Fit the stacking model $\hat{f}_{\mathrm{stack}}$ using 
$$(y_1,z_1),\ldots, (y_n,z_n)$$

5. Make final predictions $\hat{y}^{*}_i = \hat{f}_{\mathrm{stack}}(z^*_i)$, $i=1,\ldots,m$

---

# Boston data

```{r}
rm(list=ls())
library(MASS)
set.seed(123)
istrain = rbinom(n=nrow(Boston),size=1,prob=0.5)>0
train <- Boston[istrain,]
n=nrow(train)
test = Boston[!istrain,-14]
test.y = Boston[!istrain,14]
m=nrow(test)
```

The training and test data are 
$$(x_1,y_1),\ldots,(x_n,y_n),\quad (x^*_1,y^*_1),\ldots,(x^*_m,y^*_m)$$
with $n=235$ and $m=271$ for the Boston data set. 

The response variable is `medv`, and the
predictor variables are `crim, zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat` 

---

# Staked regression with two models

* Fit a library of $L=2$ models $\hat{f}_1$ and $\hat{f}_2$ to the training set.

* The first model $\hat{f}_1$ is a linear model with all predictors

```{r}
fit1 = lm(medv ~ ., train)
```

* The second model $\hat{f}_1$ is a regression tree with all predictors and default settings

```{r}
library(rpart)
fit2 = rpart(medv ~ ., train)
```

---

1. Let $\hat{f}^{-i}_l(x_i)$ be the prediction at $x_i$ using model $l$ fitted to the training data with the
$i$th training observation $(x_i,y_i)$ removed

2. Obtain the weights by least squares
$$\hat{w}_1,\ldots,\hat{w}_L = \underset{w_1,\ldots,w_L}{\arg \min} \sum_{i=1}^{n} \left[ y_i - \sum_{l=1}^{L} w_l \hat{f}^{-i}_l(x_i) \right]^2$$

3. Compute the predictions for the test set as
$$\hat{f}_{\mathrm{stack}}(x^*_i) = \sum_{l=1}^{L} \hat{w}_l  \hat{f}_l(x^*_i), \quad i=1,\ldots,m$$
4. Compute the mean squared error for the test set
$$\mathrm{MSE}^{\mathrm{stack}}_{\mathrm{Te}} = \frac{1}{m}\sum_{i=1}^{m} (y^*_i - \hat{f}_{\mathrm{stack}}(x^*_i) )^2$$ and compare with
$$\mathrm{MSE}^l_{\mathrm{Te}} = \frac{1}{m}\sum_{i=1}^{m} (y^*_i - \hat{f}_{l}(x^*_i) )^2, \quad l=1,\ldots,L.$$

---

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# 2.
z1 = vector()
z2 = z1
for (i in 1:n){
z1[i] = predict( lm(medv ~ (.), train[-i, ]), 
                 newdata = train[i,]
                 )
z2[i] = predict(rpart(medv ~ ., train[-i,]), 
                newdata = train[i,]
                ) 
}
# 3. 
fit = lm(medv ~ 0 + z1 + z2, train)
weights = coef(fit)
# 4. 
yhat1 = predict(fit1, newdata=test)
yhat2 = predict(fit2, newdata=test)
yhat = weights[1]*yhat1 + weights[2]*yhat2
# 5.
cat("MSE stack: ", mean( (test.y - yhat)^2) ) 
cat("MSE lm: ", mean( (test.y - yhat1)^2) ) 
cat("MSE rpart: ", mean( (test.y - yhat2)^2) ) 
```




---

# caretEnsemble


```{r, eval=FALSE}
# K fold CV for regression
KCV <- trainControl(method="cv", 
                    number=K,
                    savePredictions="final",
                    index=createResample(train$y, K)
                    )
# list of models fit1 and fit2
List <- caretList(y ~. ,
                  data=train,
                  trControl=KCV,
                  methodList=c("fit1","fit2")
                  )
# ensemble fit
fit.ensemble <- caretEnsemble(List, metric="RMSE")
summary(fit.ensemble)
```

* See [A Brief Introduction to caretEnsemble](https://cran.r-project.org/web/packages/caretEnsemble/vignettes/caretEnsemble-intro.html
) by Zach Mayer

---

# Boston data

```{r, eval=FALSE}
# libraries
rm(list=ls())
library(MASS)
library(caret)
library(caretEnsemble)

# import data
set.seed(123)
split <- createDataPartition(y=Boston$medv, p = 0.5, list=FALSE)
train <- Boston[split,]
test = Boston[-split,-14]
test.y = Boston[-split,14]
nrow(test)

# cross-validation settings
K = 10
my_control <- trainControl(
  method="cv",
  number=K,
  savePredictions="final",
  index=createResample(train$medv, K)
  )
```

---

# Library of models

```{r, eval=FALSE}
model_list <- caretList(
  medv~., data=train,
  methodList=c("lm","ctree"), 
  tuneList=list(
    rf=caretModelSpec(method="rf", tuneLength=3)
  ),
  trControl=my_control
)

xyplot(resamples(model_list))
modelCor(resamples(model_list))
```

---

# Ensemble

```{r, eval=FALSE}
greedy_ensemble <- caretEnsemble(
  model_list, 
  metric="RMSE"
  )
summary(greedy_ensemble)
```

---

# Test MSE

```{r, eval=FALSE}
yhats <- lapply(model_list, predict, newdata=test)
lapply(yhats, function(yhat) mean((yhat - test.y)^2) )

yhat.en <- predict(greedy_ensemble, newdata=test)
mean((yhat.en - test.y)^2)
```

